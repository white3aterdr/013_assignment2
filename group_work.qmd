---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Rookie Programmers Group Project
nocite: '@*'
execute:
  echo: false
format:
  html:
    theme:
      - minty
      - css/web.scss
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto
    monofont: JetBrainsMono-Regular
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Declaration of Authorship {.unnumbered .unlisted}

We, Rookie Programmers, confirm that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date: 2023/12/06

Student Numbers: 23148316/

## Brief Group Reflection

| What Went Well | What Was Challenging |
| -------------- | -------------------- |
| A              | B                    |
| C              | D                    |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

```{=html}
<style type="text/css">
.duedate {
  border: dotted 2px red; 
  background-color: rgb(255, 235, 235);
  height: 50px;
  line-height: 50px;
  margin-left: 40px;
  margin-right: 40px
  margin-top: 10px;
  margin-bottom: 10px;
  color: rgb(150,100,100);
  text-align: center;
}
</style>
```

{{< pagebreak >}}

# Response to Questions

```{python}
import os
import requests
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as colors
import seaborn as sns
import geopandas as gpd
from shapely.geometry import Point
from mpl_toolkits.axes_grid1 import make_axes_locatable
```

```{python}
import warnings
warnings.filterwarnings('ignore')
```

```{python}
# 确认数据文件是否存在，如无则下载
file_gz = 'listings.csv.gz'
url_gz = 'https://github.com/white3aterdr/013_assignment2/raw/main/listings.csv.gz'

if os.path.exists(file_gz):
    data = pd.read_csv(file_gz, compression='gzip', encoding='ISO-8859-1', dtype={68: str}, low_memory=False)
else: 
    data = pd.read_csv(url_gz, compression='gzip', encoding='ISO-8859-1', dtype={68: str}, low_memory=False)
    data.to_csv(file_gz)

# 确认参考文献文件是否存在，如无则下载
file_bib = "bio.bib"
url_bib = "https://github.com/white3aterdr/013_assignment2/raw/main/bio.bib"

if not os.path.exists(file_bib):
    response = requests.get(url_bib)
    with open(file_bib, "wb") as file:
        file.write(response.content)

# 确认格式文件是否存在，如无则下载
file_csl = "harvard-cite-them-right.csl"
url_csl = "https://github.com/white3aterdr/013_assignment2/raw/main/harvard-cite-them-right.csl"

if not os.path.exists(file_csl):
    response = requests.get(url_csl)
    with open(file_csl, "wb") as file:
        file.write(response.content)

# 确认地理文件是否存在，如无则下载
file_gpkg = "Boroughs.gpkg"
url_gpkg = "https://github.com/white3aterdr/013_assignment2/raw/main/Boroughs.gpkg"

if not os.path.exists(file_gpkg):
    response = requests.get(url_gpkg)
    with open(file_gpkg, "wb") as file:
        file.write(response.content)
```

## 1. Who collected the data?

The data from insideairbnb website [@noauthor_home_nodate] were collected by the Inside Airbnb project, with contributions from a variety of collaborators and partners.

The raw data were compiled from the Airbnb website [@noauthor_airbnb_nodate-1], which was collected by the company Airbnb, Inc.


## 2. Why did they collect it?

On the insideairbnb website [@noauthor_about_nodate], they write:

>"We work towards a vision where communities are empowered with data and information to understand, decide and control the role of renting residential homes to tourists".

As for the company Airbnb, three considerations may arise：
  
- Operational aspect: The primary goal is to assist users in making informed decisions-  

- Company aspect: Data collection plays a crucial role in analyzing Airbnb's operational performance across various cities. With the gathered data, Airbnb can fine-tune its operational strategies.

- Legal aspect: In compliance with diverse regulatory requirements, Airbnb must furnish information to the respective authorities as necessary.


## 3. How was the data collected?  

Insideairbnb collected data, which was a snapshot at a particular time, from Airbnb platform.

The data collected by Airbnb can be classified along two dimensions: the data source and the method of generation. 

#### From host: 

- Objective data. These data were provided by hosts but can be verified through photos or qualifications. And some data can also be automatically obtained based on user-provided GPS location. Such as listing data: latitude, longitude, property_type, room_type, etc. Host data: host_name, host_location, host_neighbourhood, etc.

- Generated during the operation of the platform. These data were generated by the platform during its operation, but its content depends on the behavior of the host. Such as host_acceptance_rate, host_response_rate, host_is_superhost, etc.

- Subjective data. These data were provided by the host and were very subjective description, and its content and tendency depend on the host's own preferences. Such as description, neighborhood_overview, host_about, etc.

#### From guest: 

- Objective data. When guest register an Airbnb account, they need to provide personal information such as name, contact details, payment information, etc.

- Generated during the operation of the platform. These data were generated by the platform, but its content depends on the behavior of the guest. Such as number_of_reviews, first_review, last_review, etc.

- Subjective data. These data were provided by the guest, depends on guest preference. Such as review_scores_accuracy, review_scores_cleanliness, comments, etc.

#### From platform: 

- Generated during the operation of the platform. These data were automatically generated by Airbnb and the user's behavior has no impact on it. Such as host_id, host_url, reviewer_id, etc. And the platform does not proactively provide objective or subjective data.


## 4. How does the method of collection impact the completeness and/or accuracy of its representation of the process it seeks to study, and what wider issues does this raise?

#### Completeness:

- For insideairbnb data, as it is a snapshot of the Airbnb platform at a particular moment, it cannot reflect changing information.

- Regarding the original data on Airbnb, if the collection relies on voluntary submissions (such as reviews from Airbnb hosts and guests), it might not be possible to gather data from all relevant individuals. Self-selection bias could lead to incomplete data, as only certain groups may be willing or able to provide information.

#### Accuracy: 

- Objective data, such as geographical location, are usually accurate and reliable.

- Data generated by the platform might be accurate but can also be subject to technical errors. However, in cases of data manipulation, such as fake reviews or artificially inflating booking numbers, there is a possibility of introducing misleading metrics for listing evaluation.

- The completeness and accuracy of subjectively provided information depend on the provider's expertise as well as their willingness to share accurate details. For hosts, varying levels of professional experience may result in significantly different expressions even for descriptions of the same listing. Hosts with less experience may overlook advantages or omit points that potential guests are concerned about. On the other hand, experienced landlords can better highlight attractive features, while those with deceptive intentions may conceal certain flaws. For guests, they come from diverse backgrounds and have different judgment criteria, leading to listing evaluations being more relative to their own perceptions. A detail highly valued by one user may go unnoticed by another, resulting in a skewed evaluation. 


## 5. What ethical considerations does the use of this data raise? 

#### Privacy and Security: 

- Privacy Concerns: Personal data, such as names, contact information, and payment details, are sensitive. How this data is collected, stored, and used is a significant privacy concern. Users must be informed about what data is collected and how it will be used, and their consent should be obtained, particularly for data that could be used to identify them. There have already been news reports that some landlords are discontent with the platform's requirement to upload photos.

- Data Security and Usage Control: Keeping data safe from unauthorized access and breaches is crucial. Ethical considerations mean implementing strong security measures to protect user data from cyber threats. Also, it's vital to make sure that data collected for one purpose isn't used inappropriately without the user's clear permission. This includes avoiding sharing data with others or using it for undisclosed reasons.

#### Bias and Discrimination: 

- The data collection process and analysis methods may inadvertently create bias or overlook certain groups. Such as the feelings and needs of groups that lack the willingness to comment.

- Data collected from user reviews or host descriptions may reflect personal biases. This could lead to discrimination against certain groups based on race, gender, age, or other factors. Ethical use of data requires mechanisms to identify and mitigate such biases.

- The Matthew effect brought about by the popularity mechanism, such as Superhosts, etc. This inequality may result in popular listings being in short supply, while unpopular listings remain uninterested.

#### Distortion and Misleading: 

- Differences in host capabilities may lead to distortions in the photos or descriptions provided, ultimately misrepresenting the true status of the listing. This discrepancy can prevent the data from accurately reflecting the information, making it challenging for guests to gain a precise understanding of the listing's quality.

- Intentional misleading, such as fake photos and fake reviews, may mislead users. It can also produce erroneous results for data analysis. This requires the platform to set up mechanisms to discover and eliminate it, and set corresponding punitive measures.

#### Impact on Communities: 

- When utilizing data for research, analysis, and descriptions of communities, it's essential to acknowledge that the produced data may only offer a partial reflection of the status of certain communities, not encompassing all aspects. However, the release of these findings can have diverse impacts on the community, including external evaluations, rent fluctuations, effects on residents' lives, and socioeconomic stratification. Therefore, careful consideration of the potential consequences arising from the use of such data is paramount.

```{python}
# 确认数据内容
# data.shape
```

```{python}
# 确认数据内容
# data.info()
```

```{python}
# 数据清洗step1 查看每列缺失值个数并按照降序排序
data.loc[:, data.isnull().sum() > 0].isnull().sum().sort_values(ascending=False);
```

```{python}
# 数据清洗step2：查看每列内容为0或基本无变化的列
data.loc[:, data.nunique() <= 1].nunique().sort_values();
```

```{python}
# 根据以上两个数据清洗代码，删除不需要的列
data = data.drop(columns=['scrape_id', 'calendar_updated', 'neighbourhood_group_cleansed', 'license', 'bathrooms'])
```

```{python}
# 过滤掉last review在2019年以前的行
# 确保 'last_review' 列是日期时间格式
data['last_review'] = pd.to_datetime(data['last_review'], errors='coerce')

# 过滤掉 'last_review' 中年份小于2019的行
data = data[data['last_review'].dt.year >= 2019]
```

```{python}
# 确认操作结果
# data.info()
```

```{python}
# 这段代码运行时间较长

# 读取伦敦区域的GeoPackage文件
gdf_boroughs = gpd.read_file('Boroughs.gpkg', layer='boroughs')

# 创建点的地理数据
geometry = [Point(xy) for xy in zip(data['longitude'], data['latitude'])]
gdf_points = gpd.GeoDataFrame(data, geometry=geometry)

# 确保点的CRS与区域的CRS相同
gdf_points.crs = "EPSG:4326"  # 假设原始点数据是WGS84
data = gdf_points.to_crs(gdf_boroughs.crs)

# 筛选Inner London的区域
selected_boroughs = [
    'Camden', 
    'Hackney', 
    'Hammersmith and Fulham', 
    'Islington', 
    'Kensington and Chelsea', 
    'Lambeth', 
    'Lewisham', 
    'Southwark', 
    'Tower Hamlets', 
    'Wandsworth', 
    'Westminster',
    'City of London'
]

# 创建一个指示每个点是否位于选定区的布尔序列
in_selected_borough = data.apply(lambda x: any(borough.contains(x.geometry) for borough in gdf_boroughs[gdf_boroughs['NAME'].isin(selected_boroughs)].geometry), axis=1)

# 创建并列的两张地图
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10), facecolor='white')  # 设置图表底色为淡米黄色

# 绘制左侧地图：未经筛选的数据
gdf_boroughs.plot(ax=ax1, color='white', edgecolor='grey', linewidth=0.6)  # 米黄色填充，灰色边界，外围边界加粗
data.plot(ax=ax1, color='deepskyblue', markersize=1, alpha=0.5)  # 增加点的透明度
ax1.axis('off')  # 不显示坐标轴

# 绘制右侧地图：筛选后的数据
gdf_boroughs.plot(ax=ax2, color='white', edgecolor='grey', linewidth=0.6)
data[in_selected_borough].plot(ax=ax2, color='deepskyblue', markersize=0.4, alpha=0.3)
ax2.axis('off')

# 设置标题
ax1.set_title('(a) All Points')
ax2.set_title('(b) Points in Selected Boroughs')

# 显示地图
plt.show()
```

```{python}
# 用过滤后的数据覆盖原有的 DataFrame
# 在这里更换变量名，避免后续更改代码导致需要从最初读取data耗费时间过长
data_2 = data[in_selected_borough].copy()
```

```{python}
# # 覆盖保存原有的 CSV 文件
# data_2.to_csv('clean_listings_1.csv', index=False)
```

```{python}
# data_2.info()
```

```{python}
# 删除之后满意度评级和热度评级中包含缺失值的行
columns_to_check = [
    'review_scores_accuracy', 
    'review_scores_cleanliness', 
    'review_scores_checkin', 
    'review_scores_communication', 
    'review_scores_location', 
    'review_scores_value',
    'last_review',
    'first_review',
    'number_of_reviews',
    'availability_60',
    'availability_30',
]

# 删除这些列中含有缺失值的行
data_2 = data_2.dropna(subset=columns_to_check)
```

```{python}
# 删除60天内无空房的列，我们认为这些listing并非订满，而是由于host设置不可预定
data_2 = data_2[data_2['availability_60'] != 0]
```

```{python}
# 维度1：热度

# 确保日期列是 datetime 类型
data_2['last_review'] = pd.to_datetime(data_2['last_review'])
data_2['first_review'] = pd.to_datetime(data_2['first_review'])

# 计算日期差并转换为天数
data_2['operating_days'] = (data_2['last_review'] - data_2['first_review']).dt.days

# 避免除以零，我们可以给分母加上一个小的数值，如 0.1，或者直接过滤掉这些行
data_2 = data_2[data_2['operating_days'] > 0]

# 计算reviews per day，考虑不可用的天数
data_2['reviews_per_day'] = data_2['number_of_reviews'] / (data_2['operating_days'])

# 删除reviews per day > 1 的数据
data_2 = data_2[data_2['reviews_per_day'] <= 1]
```

```{python}
#计算月入住率
data_2['occupancy_rate'] = ((30 - data_2['availability_30']) / 30) 

# 将 occupancy_rate 和 reviews_per_day 数值加权相加，其中评论数结合留评率概念，取10倍权重
data_2['combined_value'] = data_2['occupancy_rate'] + 10*data_2['reviews_per_day']
```

```{python}
# result_columns = ['number_of_reviews', 'operating_days', 'reviews_per_day', 'occupancy_rate', 'combined_value']
# print(data_2[result_columns].head())
```

```{python}
# 根据 'host_is_superhost' 更新 'reviews_per_day_super_host' 列的值
data_2['super_host_rate'] = data_2.apply(
    lambda row: row['combined_value'] + 0.5 if row['host_is_superhost'] == 't' else row['combined_value'],
    axis=1
)
```

```{python}
# print(data_2[['host_is_superhost', 'combined_value', 'super_host_rate']].head())
```

```{python}
# data_2['reviews_per_day'].describe()
```

```{python}
# data_2['occupancy_rate'].describe()
```

```{python}
# data_2['super_host_rate'].describe()
```

```{python}
# 假设使用固定数值作为阈值来分热度，这里我们需要五个阈值分成六个区间
dimension_1 = data_2.sort_values(by='super_host_rate',ascending=False)


thresholds = [dimension_1.super_host_rate.quantile(0.00),
              dimension_1.super_host_rate.quantile(0.20), 
              dimension_1.super_host_rate.quantile(0.40), 
              dimension_1.super_host_rate.quantile(0.60), 
              dimension_1.super_host_rate.quantile(0.80), 
              dimension_1.super_host_rate.quantile(1.00)]  # 举例的阈值
labels = [1, 2, 3, 4, 5]  # 五个热度标签

# 分配热度
data_2['star_rating'] = pd.cut(data_2['super_host_rate'], bins=thresholds, labels=labels, include_lowest=True)
```

```{python}
# 假设热度已经被计算并分配到 'star_rating' 列中
star_rating_counts = data_2['star_rating'].value_counts()

# 按照热度排序，如果有必要的话
star_rating_counts_sorted = star_rating_counts.sort_index()

# 打印热度个数统计
# print(star_rating_counts_sorted)
```

```{python}
# 维度2：满意度
# 加权依据：https://medium.com/@labdmitriy/exploring-airbnb-guest-reviews-in-london-682b45aba34e中问题1和问题2的结论
# 定义加权平均的权重
weights = {
    'review_scores_accuracy': 1.5,
    'review_scores_cleanliness': 2,
    'review_scores_checkin': 1,
    'review_scores_communication': 1,
    'review_scores_location': 0.5,
    'review_scores_value': 1
}

# 定义评分列的列表
score_columns = [
    'review_scores_accuracy', 
    'review_scores_cleanliness', 
    'review_scores_checkin', 
    'review_scores_communication', 
    'review_scores_location', 
    'review_scores_value'
]
# 将评分列转换为浮点数
for col in score_columns:
    data_2[col] = pd.to_numeric(data_2[col], errors='coerce')  # 将无法转换的值设置为 NaN

# 计算加权平均分的函数
def weighted_average(row):
    total_score = sum(row[col] * weights[col] for col in score_columns if not pd.isna(row[col]))
    total_weight = sum(weights[col] for col in score_columns if not pd.isna(row[col]))
    return total_score / total_weight if total_weight > 0 else None

# 应用加权平均分计算函数
data_2['weighted_score'] = data_2.apply(weighted_average, axis=1)
sorted_data_descending = data_2.sort_values(by='weighted_score',ascending=False)

# 定义满意度等级的函数
def satisfaction_level(weighted_score):
    if weighted_score >= sorted_data_descending.weighted_score.quantile(0.80) :
        return 5
    elif weighted_score >= sorted_data_descending.weighted_score.quantile(0.60):
        return 4
    elif weighted_score >= sorted_data_descending.weighted_score.quantile(0.40):
        return 3
    elif weighted_score >= sorted_data_descending.weighted_score.quantile(0.20):
        return 2
    else:
        return 1

# 应用满意度等级计算函数
data_2['satisfaction_level'] = data_2['weighted_score'].apply(satisfaction_level)

# 显示所有的满意度等级
#print(data_2['satisfaction_level'].tolist())
# print(data_2['satisfaction_level'].value_counts())
```

```{python}
# # 在本地IDE中生成名为Clean_listings的数据清洗整理和评级后的csv
# corrected_classified_file_name = 'clean_listings_2.csv'
# data_2.to_csv(corrected_classified_file_name, index=False)

# # Print the file save path
# print(f"Corrected classified listings file saved as: {corrected_classified_file_name}")
```

```{python}
# 加载伦敦区域的GeoPackage文件
gdf_boroughs_new = gpd.read_file('Boroughs.gpkg', layer='boroughs')
geometry = [Point(xy) for xy in zip(data_2['longitude'], data_2['latitude'])]
gdf_points_new = gpd.GeoDataFrame(data_2, geometry=geometry)

# 设置坐标参考系统(CRS)
gdf_points_new.crs = "EPSG:4326"  # 假设原始点数据是WGS84
gdf_points_new = gdf_points_new.to_crs(gdf_boroughs_new.crs)
```

```{python}
geometry = [Point(xy) for xy in zip(data_2['longitude'], data_2['latitude'])]
gdf_quality = gpd.GeoDataFrame(data_2, geometry=geometry)

# 设置坐标参考系统(CRS)
gdf_quality.crs = "EPSG:4326"
gdf_quality = gdf_quality.to_crs(gdf_boroughs_new.crs)
```

```{python}
# 创建包含两个子图的画布
fig, axs = plt.subplots(1, 2, figsize=(20, 10))

# 调整子图之间的间距来为图例留出空间
plt.subplots_adjust(wspace=0.3)

# 第一个子图：热度评分
gdf_boroughs_new.plot(ax=axs[0], color='white', edgecolor='lightgrey')
scatter1 = gdf_points_new.plot(ax=axs[0], column='star_rating', cmap='viridis', markersize=3)

# 第二个子图：满意度等级
gdf_boroughs_new.plot(ax=axs[1], color='white', edgecolor='lightgrey')
scatter2 = gdf_quality.plot(ax=axs[1], column='satisfaction_level', cmap='viridis', markersize=3)

# 为图例创建一个 ScalarMappable 对象
norm = colors.Normalize(vmin=gdf_points_new['star_rating'].min(), vmax=gdf_points_new['star_rating'].max())
sm1 = plt.cm.ScalarMappable(cmap='viridis', norm=norm)
sm1.set_array([])

# 添加第一个子图的图例
divider = make_axes_locatable(axs[0])
cax = divider.append_axes("right", size="5%", pad=0.1)
fig.colorbar(sm1, cax=cax)

# 为第二个图例重复相同步骤
norm = colors.Normalize(vmin=gdf_quality['satisfaction_level'].min(), vmax=gdf_quality['satisfaction_level'].max())
sm2 = plt.cm.ScalarMappable(cmap='viridis', norm=norm)
sm2.set_array([])

divider = make_axes_locatable(axs[1])
cax = divider.append_axes("right", size="5%", pad=0.1)
fig.colorbar(sm2, cax=cax)

# 设置标题
axs[0].set_title('(a) Heat Levels in London')
axs[1].set_title('(b) Satisfaction Levels in London')

# 调整显示范围以只显示点的分布区域
xmin, ymin, xmax, ymax = gdf_points_new.total_bounds
axs[0].set_xlim([xmin, xmax])
axs[0].set_ylim([ymin, ymax])

xmin, ymin, xmax, ymax = gdf_quality.total_bounds
axs[1].set_xlim([xmin, xmax])
axs[1].set_ylim([ymin, ymax])

# 显示结果
plt.show()
```

```{python}
# 筛选出满足条件的点
red_points = gdf_points_new[(gdf_points_new['star_rating'] == 5) & (gdf_points_new['satisfaction_level'] == 1)]
red_points_all = gdf_points_new[(gdf_points_new['star_rating'] == 5)]
cyan_points = gdf_points_new[(gdf_points_new['star_rating'] == 1) & (gdf_points_new['satisfaction_level'] == 5)]
cyan_points_all = gdf_points_new[(gdf_points_new['satisfaction_level'] == 5)]

# Create the figure and subplots
fig, axs = plt.subplots(1, 2, figsize=(20, 10), sharex=True, sharey=True)

# Plot the boroughs as the base layer for both subplots
gdf_boroughs.plot(ax=axs[0], color='white', edgecolor='lightgrey')
gdf_boroughs.plot(ax=axs[1], color='white', edgecolor='lightgrey')

# Plot the red and cyan points on their respective subplots
red_points_all.plot(ax=axs[0], color='mistyrose', markersize=3)
red_points.plot(ax=axs[0], color='red', markersize=3)
cyan_points_all.plot(ax=axs[1], color='lightblue', markersize=3)
cyan_points.plot(ax=axs[1], color='dodgerblue', markersize=3)

# Set the titles for both subplots
axs[0].set_title('(c) "Heat5" with highlighted "Satisfaction1"')
axs[1].set_title('(d) "Heat1" with highlighted "Satisfaction5"')

# Determine the combined bounds of both datasets
xmin = min(red_points.total_bounds[0], cyan_points.total_bounds[0])
ymin = min(red_points.total_bounds[1], cyan_points.total_bounds[1])
xmax = max(red_points.total_bounds[2], cyan_points.total_bounds[2])
ymax = max(red_points.total_bounds[3], cyan_points.total_bounds[3])

# 调整显示范围以只显示点的分布区域
xmin, ymin, xmax, ymax = gdf_points_new.total_bounds
axs[0].set_xlim([xmin, xmax])
axs[0].set_ylim([ymin, ymax])

xmin, ymin, xmax, ymax = gdf_quality.total_bounds
axs[1].set_xlim([xmin, xmax])
axs[1].set_ylim([ymin, ymax])

# Ensure the aspect ratio is the same for both subplots
axs[0].set_aspect('equal', adjustable='box')
axs[1].set_aspect('equal', adjustable='box')

# Show the plots
plt.show()
```

```{python}
# print(red_points.count())
# print(red_points_all.count())
# print(cyan_points.count())
# print(cyan_points_all.count())
```

```{python}
# red_points['price'] = red_points['price'].replace('[\$,]', '', regex=True).astype(float)
# print(red_points['price'].describe())

# red_points_all['price'] = red_points_all['price'].replace('[\$,]', '', regex=True).astype(float)
# print(red_points_all['price'].describe())

# cyan_points['price'] = cyan_points['price'].replace('[\$,]', '', regex=True).astype(float)
# print(cyan_points['price'].describe())

# cyan_points_all['price'] = cyan_points_all['price'].replace('[\$,]', '', regex=True).astype(float)
# print(cyan_points_all['price'].describe())
```

```{python}
# # 筛选出满足条件的点
# red_points = gdf_points_new[(gdf_points_new['star_rating'] == 5) & (gdf_points_new['satisfaction_level'] == 1)]
# cyan_points = gdf_points_new[(gdf_points_new['star_rating'] == 1) & (gdf_points_new['satisfaction_level'] == 5)]

# # 创建画布
# fig, ax = plt.subplots(figsize=(10, 10))

# # 绘制点
# gdf_boroughs.plot(ax=ax, color='white', edgecolor='lightgrey')
# red_points.plot(ax=ax, color='red', markersize=10, label='Heat: 5, Quality: 1')
# cyan_points.plot(ax=ax, color='cyan', markersize=10, label='Heat: 1, Quality: 5')

# # 设置标题
# ax.set_title('Special Points in London')

# # 调整显示范围
# xmin, ymin, xmax, ymax = gdf_points_new.total_bounds
# ax.set_xlim([xmin, xmax])
# ax.set_ylim([ymin, ymax])

# # 添加图例
# ax.legend()

# # 显示结果
# plt.show()
```

```{python}
# print(gdf_quality['satisfaction_level'].describe())
```

```{python}
# print(gdf_points_new['star_rating'].describe())
```

```{python}
# 
```

```{python}
data_2.info()
```

```{python}
from sklearn.preprocessing import OneHotEncoder # We don't use this but I point out where you *could*
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
```

```{python}
import nltk
import spacy
from nltk.corpus import wordnet as wn
from nltk.stem.wordnet import WordNetLemmatizer

from nltk.corpus import stopwords

from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tokenize.toktok import ToktokTokenizer

from nltk.stem.porter import PorterStemmer
from nltk.stem.snowball import SnowballStemmer

from nltk import ngrams, FreqDist

lemmatizer = WordNetLemmatizer()
tokenizer = ToktokTokenizer()
```

```{python}
# the method in the practical-07
import urllib.request
host  = 'https://orca.casa.ucl.ac.uk'
turl  = f'{host}/~jreades/__textual__.py'
tdirs = os.path.join('textual')
tpath = os.path.join(tdirs,'__init__.py')

if not os.path.exists(tpath):
    os.makedirs(tdirs, exist_ok=True)
    urllib.request.urlretrieve(turl, tpath)
```

```{python}
from textual import *
```

```{python}
# select the houses which is 5 in star_rating and satisfaction_level
Rank5_house = data_2[data_2['star_rating'] == 5]
satisfaction5_house = data_2[data_2['satisfaction_level'] == 5]
#normalise the text
#Rank5_house['description'] = Rank5_house.description.apply(normalise_document, remove_digits=True)
analyzeRank5 = Rank5_house

# select the houses which is 4 in star_rating and satisfaction_level
Rank4_house = data_2[data_2['star_rating'] == 4]
satisfaction4_house = data_2[data_2['satisfaction_level'] == 4]
#normalise the text
#Rank4_house['description'] = Rank4_house.description.apply(normalise_document, remove_digits=True)
analyzeRank4 = Rank4_house

# select the houses which is 3 in star_rating and satisfaction_level
Rank3_house = data_2[data_2['star_rating'] == 3]
satisfaction3_house = data_2[data_2['satisfaction_level'] == 3]
#normalise the text
#Rank3_house['description'] = Rank3_house.description.apply(normalise_document, remove_digits=True)
analyzeRank3 = Rank3_house

# select the houses which is 2 in star_rating and satisfaction_level
Rank2_house = data_2[data_2['star_rating'] == 2]
satisfaction2_house = data_2[data_2['satisfaction_level'] == 2]
#normalise the text
#Rank2_house['description'] = Rank2_house.description.apply(normalise_document, remove_digits=True)
analyzeRank2 = Rank2_house

# select the houses which is 1 in star_rating and satisfaction_level
Rank1_house = data_2[data_2['star_rating'] == 1]
satisfaction1_house = data_2[data_2['satisfaction_level'] == 1]
#normalise the text
#Rank1_house['description'] = Rank1_house.description.apply(normalise_document, remove_digits=True)
analyzeRank1 = Rank1_house


highRank_lowSat = data_2[(data_2['satisfaction_level']) == 1 & (data_2['star_rating'] == 5)]
lowRank_highSat = data_2[(data_2['satisfaction_level'])== 5 & (data_2['star_rating'] == 1)]
```

```{python}
RankAll = [Rank1_house,Rank2_house,Rank3_house,Rank4_house,Rank5_house]
satisfactionAll = [satisfaction1_house,satisfaction2_house,satisfaction3_house,satisfaction4_house,satisfaction5_house]
extreme = [highRank_lowSat, lowRank_highSat]
```

```{python}
#Calculate the average of host_response_rate
host_response_rate = []
for i in RankAll:
    j = i.dropna(subset=['host_response_rate'])
    mean = j['host_response_rate'].str.rstrip('%').astype('float') / 100.0
    host_response_rate.append(mean.mean())

for i in satisfactionAll:
    j = i.dropna(subset=['host_response_rate'])
    mean = j['host_response_rate'].str.rstrip('%').astype('float') / 100.0
    host_response_rate.append(mean.mean())
    
for i in extreme:
    j = i.dropna(subset=['host_response_rate'])
    mean = j['host_response_rate'].str.rstrip('%').astype('float') / 100.0
    host_response_rate.append(mean.mean())

```

```{python}
#Calculate the average of host_acceptance_rate
host_acceptance_rate = []
for i in RankAll:
    j = i.dropna(subset=['host_acceptance_rate'])
    mean = j['host_acceptance_rate'].str.rstrip('%').astype('float') / 100.0
    host_acceptance_rate.append(mean.mean())

for i in satisfactionAll:
    j = i.dropna(subset=['host_acceptance_rate'])
    mean = j['host_acceptance_rate'].str.rstrip('%').astype('float') / 100.0
    host_acceptance_rate.append(mean.mean())

for i in extreme:
    j = i.dropna(subset=['host_acceptance_rate'])
    mean = j['host_acceptance_rate'].str.rstrip('%').astype('float') / 100.0
    host_acceptance_rate.append(mean.mean())

    
```

```{python}
# Calculate the average length of neighborhood_overview text
neighborhood_overview = []
for i in RankAll:
    j = i.dropna(subset=['neighborhood_overview'])
    j['neighborhood_overview'] = j.neighborhood_overview.apply(normalise_document, remove_digits=True)
    textlist = j.neighborhood_overview.tolist()
    total = 0
    for i in textlist:
        total = total + len(i)
    averge = total/len(textlist)
    neighborhood_overview.append(averge)

for i in satisfactionAll:
    j = i.dropna(subset=['neighborhood_overview'])
    j['neighborhood_overview'] = j.neighborhood_overview.apply(normalise_document, remove_digits=True)
    textlist = j.neighborhood_overview.tolist()
    total = 0
    for i in textlist:
        total = total + len(i)
    averge = total/len(textlist)
    neighborhood_overview.append(averge)

for i in extreme:
    j = i.dropna(subset=['neighborhood_overview'])
    j['neighborhood_overview'] = j.neighborhood_overview.apply(normalise_document, remove_digits=True)
    textlist = j.neighborhood_overview.tolist()
    total = 0
    for i in textlist:
        total = total + len(i)
    averge = total/len(textlist)
    neighborhood_overview.append(averge)

```

```{python}
# Calculate the average length of description text
description = []
for i in RankAll:
    j = i.dropna(subset=['description'])
    j['description'] = j.description.apply(normalise_document, remove_digits=True)
    textlist = j.description.tolist()
    total = 0
    for i in textlist:
        total = total + len(i)
    averge = total/len(textlist)
    description.append(averge)

for i in satisfactionAll:
    j = i.dropna(subset=['description'])
    j['description'] = j.description.apply(normalise_document, remove_digits=True)
    textlist = j.description.tolist()
    total = 0
    for i in textlist:
        total = total + len(i)
    averge = total/len(textlist)
    description.append(averge)

for i in extreme:
    j = i.dropna(subset=['description'])
    j['description'] = j.description.apply(normalise_document, remove_digits=True)
    textlist = j.description.tolist()
    total = 0
    for i in textlist:
        total = total + len(i)
    averge = total/len(textlist)
    description.append(averge)
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
# Calculate the average length of host_about text
host_about = []
for i in RankAll:
    j = i.dropna(subset=['host_about'])
    j['host_about'] = j.host_about.apply(normalise_document, remove_digits=True)
    textlist = j.host_about.tolist()
    total = 0
    for i in textlist:
        total = total + len(i)
    averge = total/len(textlist)
    host_about.append(averge)

for i in satisfactionAll:
    j = i.dropna(subset=['host_about'])
    j['host_about'] = j.host_about.apply(normalise_document, remove_digits=True)
    textlist = j.host_about.tolist()
    total = 0
    for i in textlist:
        total = total + len(i)
    averge = total/len(textlist)
    host_about.append(averge)

for i in extreme:
    j = i.dropna(subset=['host_about'])
    j['host_about'] = j.host_about.apply(normalise_document, remove_digits=True)
    textlist = j.host_about.tolist()
    total = 0
    for i in textlist:
        total = total + len(i)
    averge = total/len(textlist)
    host_about.append(averge)
```

```{python}
#Calculate the average of price
price= []
for i in RankAll:
    i['price'] = i['price'].str.replace('$','', regex=False).str.replace(',','', regex=False).astype('float')
    mean =  i.price.mean()
    price.append(mean)

for i in satisfactionAll:
    i['price'] = i['price'].str.replace('$','', regex=False).str.replace(',','', regex=False).astype('float')
    mean =  i.price.mean()
    price.append(mean)

for i in extreme:
    i['price'] = i['price'].str.replace('$','', regex=False).str.replace(',','', regex=False).astype('float')
    mean =  i.price.mean()
    price.append(mean)
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
row_list = ['Rank1_house','Rank2_house','Rank3_house','Rank4_house','Rank5_house',
            'satisfaction1_house','satisfaction2_house','satisfaction3_house','satisfaction4_house','satisfaction5_house',
           'highRank_lowSat', 'lowRank_highSat']

dir = {'list_name': row_list,
        'host_response_rate_mean': host_response_rate,
        'host_acceptance_rate_mean': host_acceptance_rate,
        'neighborhood_overview_mean': neighborhood_overview,
        'description_mean': description,
        'host_about_mean': host_about,
        'price_mean': price}
```

```{python}
result_df = pd.DataFrame(dir)
```

```{python}
result_df.set_index('list_name', inplace=True)
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
result_df
```

## 6. With reference to the data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London? 



## 7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how *could* this data set be used to inform the regulation of Short-Term Lets (STL) in London? 



## References
