---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Rookie Programmers Group Project
nocite: '@*'
execute:
  echo: false
format:
  html:
    theme:
      - minty
      - css/web.scss
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto
    monofont: JetBrainsMono-Regular
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Declaration of Authorship {.unnumbered .unlisted}

We, Rookie Programmers, confirm that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date: 2023/12/11

Student Numbers: 23148316/23100260/23049577/23202303

## Brief Group Reflection

| What Went Well | What Was Challenging |
| -------------- | -------------------- |
| Great collaboration and division of labor within the group | It was difficult to select topics and find other data sets. In the end, we chose to only use the listing.csv data, and it was difficult to evaluate whether a topic was suitable before completing it. |
| The pace of work is good and it is basically completed about a week ahead of schedule. | Initially we hoped to analyze keywords, but did not get results, and we did not have the sophisticated technology of NLP. We adjusted the direction, tried to analyze other aspects, and obtained some conclusions, but we did not meet expectations. |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

There are two aspects.

We attempted to develop two rating criteria in our analysis, which required defining weights, for which there are few references. In the urban planning industry, when similar situations are involved, definitions are sometimes defined by inviting experts to score. So for group work like ours, can we first find reasonable indicators based on current conditions, and adopt similar methods to optimize in the future if necessary and conditional?

In our analysis, we initially wanted to use NLP to analyze subjective descriptive text. However, in practice, we found that only using keyword frequency analysis cannot draw effective conclusions, and the skills we have mastered are not enough to complete it. Although we have adjusted the research direction later, what should we do if in some cases, a research does not reach a conclusion of obvious significance in the end? What kind of evaluation will it get?

```{=html}
<style type="text/css">
.duedate {
  border: dotted 2px red; 
  background-color: rgb(255, 235, 235);
  height: 50px;
  line-height: 50px;
  margin-left: 40px;
  margin-right: 40px
  margin-top: 10px;
  margin-bottom: 10px;
  color: rgb(150,100,100);
  text-align: center;
}
</style>
```

{{< pagebreak >}}

# Response to Questions

```{python}
import os
import requests
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as colors
import seaborn as sns
import geopandas as gpd
from shapely.geometry import Point
from mpl_toolkits.axes_grid1 import make_axes_locatable
```

```{python}
import warnings
warnings.filterwarnings('ignore')
```

```{python}
# Check if the data file exists, if not, download it
file_gz = 'listings.csv.gz'
url_gz = 'https://github.com/white3aterdr/013_assignment2/raw/main/listings.csv.gz'

if os.path.exists(file_gz):
    data = pd.read_csv(file_gz, compression='gzip', encoding='ISO-8859-1', dtype={68: str}, low_memory=False)
else: 
    data = pd.read_csv(url_gz, compression='gzip', encoding='ISO-8859-1', dtype={68: str}, low_memory=False)
    data.to_csv(file_gz)

# Check if the bibliography file exists, if not, download it
file_bib = "bio.bib"
url_bib = "https://github.com/white3aterdr/013_assignment2/raw/main/bio.bib"

if not os.path.exists(file_bib):
    response = requests.get(url_bib)
    with open(file_bib, "wb") as file:
        file.write(response.content)

# Check if the style file exists, if not, download it
file_csl = "harvard-cite-them-right.csl"
url_csl = "https://github.com/white3aterdr/013_assignment2/raw/main/harvard-cite-them-right.csl"

if not os.path.exists(file_csl):
    response = requests.get(url_csl)
    with open(file_csl, "wb") as file:
        file.write(response.content)

# Check if the geographic file exists, if not, download it
file_gpkg = "Boroughs.gpkg"
url_gpkg = "https://github.com/white3aterdr/013_assignment2/raw/main/Boroughs.gpkg"

if not os.path.exists(file_gpkg):
    response = requests.get(url_gpkg)
    with open(file_gpkg, "wb") as file:
        file.write(response.content)

# Check if the csv file exists, if not, download it
file1_csv = "formatted_df1.csv"
url1_csv = "https://github.com/white3aterdr/013_assignment2/raw/main/formatted_df1.csv"

if not os.path.exists(file1_csv):
    response = requests.get(url1_csv)
    with open(file1_csv, "wb") as file:
        file.write(response.content)

# Check if the csv file exists, if not, download it
file2_csv = "formatted_df2.csv"
url2_csv = "https://github.com/white3aterdr/013_assignment2/raw/main/formatted_df2.csv"

if not os.path.exists(file2_csv):
    response = requests.get(url2_csv)
    with open(file2_csv, "wb") as file:
        file.write(response.content)
```

## 1. Who collected the data?

The data from insideairbnb website [@noauthor_home_nodate] were collected by the Inside Airbnb project, with contributions from a variety of collaborators and partners.

The raw data were compiled from the Airbnb website [@noauthor_airbnb_nodate-1], which was collected by the company Airbnb, Inc.


## 2. Why did they collect it?

On the insideairbnb website [@noauthor_about_nodate], they write:

>"We work towards a vision where communities are empowered with data and information to understand, decide and control the role of renting residential homes to tourists".

As for the company Airbnb, three considerations may arise：
  
- Operational aspect: The primary goal is to assist users in making informed decisions-  

- Company aspect: Data collection plays a crucial role in analyzing Airbnb's operational performance across various cities. With the gathered data, Airbnb can fine-tune its operational strategies.

- Legal aspect: In compliance with diverse regulatory requirements, Airbnb must furnish information to the respective authorities as necessary.


## 3. How was the data collected?  

Insideairbnb collected data, which was a snapshot at a particular time, from Airbnb platform.

The data collected by Airbnb can be classified along two dimensions: the data source and the method of generation. 

#### From host: 

- Objective data. These data were provided by hosts but can be verified through photos or qualifications. And some data can also be automatically obtained based on user-provided GPS location. Such as listing data: latitude, longitude, property_type, room_type, etc. Host data: host_name, host_location, host_neighbourhood, etc.

- Generated during the operation of the platform. These data were generated by the platform during its operation, but its content depends on the behavior of the host. Such as host_acceptance_rate, host_response_rate, host_is_superhost, etc.

- Subjective data. These data were provided by the host and were very subjective description, and its content and tendency depend on the host's own preferences. Such as description, neighborhood_overview, host_about, etc.

#### From guest: 

- Objective data. When guest register an Airbnb account, they need to provide personal information such as name, contact details, payment information, etc.

- Generated during the operation of the platform. These data were generated by the platform, but its content depends on the behavior of the guest. Such as number_of_reviews, first_review, last_review, etc.

- Subjective data. These data were provided by the guest, depends on guest preference. Such as review_scores_accuracy, review_scores_cleanliness, comments, etc.

#### From platform: 

- Generated during the operation of the platform. These data were automatically generated by Airbnb and the user's behavior has no impact on it. Such as host_id, host_url, reviewer_id, etc. And the platform does not proactively provide objective or subjective data.


## 4. How does the method of collection impact the completeness and/or accuracy of its representation of the process it seeks to study, and what wider issues does this raise?

#### Completeness:

- For insideairbnb data, as it is a snapshot of the Airbnb platform at a particular moment, it cannot reflect changing information.

- Regarding the original data on Airbnb, if the collection relies on voluntary submissions (such as reviews from Airbnb hosts and guests), it might not be possible to gather data from all relevant individuals. Self-selection bias could lead to incomplete data, as only certain groups may be willing or able to provide information.

#### Accuracy: 

- Objective data, such as geographical location, are usually accurate and reliable.

- Data generated by the platform might be accurate but can also be subject to technical errors. However, in cases of data manipulation, such as fake reviews or artificially inflating booking numbers, there is a possibility of introducing misleading metrics for listing evaluation.

- The completeness and accuracy of subjectively provided information depend on the provider's expertise as well as their willingness to share accurate details. For hosts, varying levels of professional experience may result in significantly different expressions even for descriptions of the same listing. Hosts with less experience may overlook advantages or omit points that potential guests are concerned about. On the other hand, experienced landlords can better highlight attractive features, while those with deceptive intentions may conceal certain flaws. For guests, they come from diverse backgrounds and have different judgment criteria, leading to listing evaluations being more relative to their own perceptions. A detail highly valued by one user may go unnoticed by another, resulting in a skewed evaluation. 


## 5. What ethical considerations does the use of this data raise? 

#### Privacy and Security: 

- Privacy Concerns: Personal data, such as names, contact information, and payment details, are sensitive. How this data is collected, stored, and used is a significant privacy concern. Users must be informed about what data is collected and how it will be used, and their consent should be obtained, particularly for data that could be used to identify them. There have already been news reports that some landlords are discontent with the platform's requirement to upload photos.

- Data Security and Usage Control: Keeping data safe from unauthorized access and breaches is crucial. Ethical considerations mean implementing strong security measures to protect user data from cyber threats. Also, it's vital to make sure that data collected for one purpose isn't used inappropriately without the user's clear permission. This includes avoiding sharing data with others or using it for undisclosed reasons.

#### Bias and Discrimination: 

- The data collection process and analysis methods may inadvertently create bias or overlook certain groups. Such as the feelings and needs of groups that lack the willingness to comment.

- Data collected from user reviews or host descriptions may reflect personal biases. This could lead to discrimination against certain groups based on race, gender, age, or other factors. Ethical use of data requires mechanisms to identify and mitigate such biases.

- The Matthew effect brought about by the popularity mechanism, such as Superhosts, etc. This inequality may result in popular listings being in short supply, while unpopular listings remain uninterested.

#### Distortion and Misleading: 

- Differences in host capabilities may lead to distortions in the photos or descriptions provided, ultimately misrepresenting the true status of the listing. This discrepancy can prevent the data from accurately reflecting the information, making it challenging for guests to gain a precise understanding of the listing's quality.

- Intentional misleading, such as fake photos and fake reviews, may mislead users. It can also produce erroneous results for data analysis. This requires the platform to set up mechanisms to discover and eliminate it, and set corresponding punitive measures.

#### Impact on Communities: 

- When utilizing data for research, analysis, and descriptions of communities, it's essential to acknowledge that the produced data may only offer a partial reflection of the status of certain communities, not encompassing all aspects. However, the release of these findings can have diverse impacts on the community, including external evaluations, rent fluctuations, effects on residents' lives, and socioeconomic stratification. Therefore, careful consideration of the potential consequences arising from the use of such data is paramount.


```{python}
# data.shape
```

```{python}
# data.info()
```

```{python}
# Data cleaning step1: Check the number of missing values ​​in each column and sort them in descending order
data.loc[:, data.isnull().sum() > 0].isnull().sum().sort_values(ascending=False);
```

```{python}
# Data cleaning step 2: Check the columns whose content is 0 or basically unchanged
data.loc[:, data.nunique() <= 1].nunique().sort_values();
```

```{python}
# Clean the data based on the above results and delete unnecessary columns
data = data.drop(columns=['scrape_id', 'calendar_updated', 'neighbourhood_group_cleansed', 'license', 'bathrooms'])
```

```{python}
# Make sure the 'last_review' column is in datetime format
data['last_review'] = pd.to_datetime(data['last_review'], errors='coerce')

# Filter out rows in 'last_review' with years less than 2019
data = data[data['last_review'].dt.year >= 2019]
```

```{python}
# data.info()
```

```{python}
# This code takes a long time to run

# Read the GeoPackage file for London boroughs
gdf_boroughs = gpd.read_file('Boroughs.gpkg', layer='boroughs')

# Create geographical data for points
geometry = [Point(xy) for xy in zip(data['longitude'], data['latitude'])]
gdf_points = gpd.GeoDataFrame(data, geometry=geometry)

# Ensure the CRS of the points is the same as the map's CRS
gdf_points.crs = "EPSG:4326"
data = gdf_points.to_crs(gdf_boroughs.crs)

# Boroughs to be retained
selected_boroughs = [
    'Camden', 
    'Hackney', 
    'Hammersmith and Fulham', 
    'Islington', 
    'Kensington and Chelsea', 
    'Lambeth', 
    'Lewisham', 
    'Southwark', 
    'Tower Hamlets', 
    'Wandsworth', 
    'Westminster',
    'City of London'
]

# Create a Boolean series indicating whether each point is within the selected boroughs
in_selected_borough = data.apply(lambda x: any(borough.contains(x.geometry) for borough in gdf_boroughs[gdf_boroughs['NAME'].isin(selected_boroughs)].geometry), axis=1)

# Create two side-by-side maps
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10), facecolor='white') 

# Plot the left map: Unfiltered data
gdf_boroughs.plot(ax=ax1, color='white', edgecolor='grey', linewidth=0.6)
data.plot(ax=ax1, color='deepskyblue', markersize=1, alpha=0.5)
ax1.axis('off')

# Plot the right map: Filtered data
gdf_boroughs.plot(ax=ax2, color='white', edgecolor='grey', linewidth=0.6)
data[in_selected_borough].plot(ax=ax2, color='deepskyblue', markersize=0.4, alpha=0.3)
ax2.axis('off')

# Set titles
ax1.set_title('(a) All Points')
ax2.set_title('(b) Points in Selected Boroughs')

# Show the map
plt.show()
```

```{python}
# Change the variable name here to avoid subsequent changes to the code that will take too long to read the data from the beginning.
data_2 = data[in_selected_borough].copy()
```

```{python}
# # save csv file
# data_2.to_csv('clean_listings_1.csv', index=False)
```

```{python}
# data_2.info()
```

```{python}
# Find rows that contain missing values ​​in satisfaction ratings and popularity ratings
columns_to_check = [
    'review_scores_accuracy', 
    'review_scores_cleanliness', 
    'review_scores_checkin', 
    'review_scores_communication', 
    'review_scores_location', 
    'review_scores_value',
    'last_review',
    'first_review',
    'number_of_reviews',
    'availability_60',
    'availability_30',
]

# Delete rows with missing values ​​in these columns
data_2 = data_2.dropna(subset=columns_to_check)
```

```{python}
# Delete the columns that have no vacancies within 60 days. We believe that these listings are not fully booked, but cannot be booked due to the host settings.
data_2 = data_2[data_2['availability_60'] != 0]
```

```{python}
# Dimension 1: Popularity

# Ensure the date columns are in datetime format
data_2['last_review'] = pd.to_datetime(data_2['last_review'])
data_2['first_review'] = pd.to_datetime(data_2['first_review'])

# Calculate the difference between dates and convert to days
data_2['operating_days'] = (data_2['last_review'] - data_2['first_review']).dt.days

# To avoid division by zero, we can add a small number to the denominator, like 0.1. Here, we choose to filter out these rows instead
data_2 = data_2[data_2['operating_days'] > 0]

# Calculate reviews per day
data_2['reviews_per_day'] = data_2['number_of_reviews'] / (data_2['operating_days'])

# Remove data where reviews per day > 1
data_2 = data_2[data_2['reviews_per_day'] <= 1]
```

```{python}
# Calculate monthly occupancy rate
data_2['occupancy_rate'] = ((30 - data_2['availability_30']) / 30) 

# Add the weighted values ​​of occupancy_rate and reviews_per_day, where the number of comments is combined with the concept of review rate, and is weighted 10 times
data_2['combined_value'] = data_2['occupancy_rate'] + 10*data_2['reviews_per_day']
```

```{python}
# result_columns = ['number_of_reviews', 'operating_days', 'reviews_per_day', 'occupancy_rate', 'combined_value']
# print(data_2[result_columns].head())
```

```{python}
# Update the value of the 'reviews_per_day_super_host' column based on 'host_is_superhost'
data_2['super_host_rate'] = data_2.apply(
    lambda row: row['combined_value'] + 0.5 if row['host_is_superhost'] == 't' else row['combined_value'],
    axis=1
)
```

```{python}
# print(data_2[['host_is_superhost', 'combined_value', 'super_host_rate']].head())
```

```{python}
# data_2['reviews_per_day'].describe()
```

```{python}
# data_2['occupancy_rate'].describe()
```

```{python}
# data_2['super_host_rate'].describe()
```

```{python}
# Assume that a fixed value is used as a threshold to classify popularity. Here we need five thresholds divided into six intervals.
dimension_1 = data_2.sort_values(by='super_host_rate',ascending=False)

thresholds = [dimension_1.super_host_rate.quantile(0.00),
              dimension_1.super_host_rate.quantile(0.20), 
              dimension_1.super_host_rate.quantile(0.40), 
              dimension_1.super_host_rate.quantile(0.60), 
              dimension_1.super_host_rate.quantile(0.80), 
              dimension_1.super_host_rate.quantile(1.00)]  # Example threshold
labels = [1, 2, 3, 4, 5]  # Five  popularity tags

# Distribute popularity
data_2['heat_level'] = pd.cut(data_2['super_host_rate'], bins=thresholds, labels=labels, include_lowest=True)
```

```{python}
# Assuming the popularity has already been calculated and assigned to the 'heat_level' column
heat_level_counts = data_2['heat_level'].value_counts()

# Sort by heat level
heat_level_counts_sorted = heat_level_counts.sort_index()

# Print the count statistics for each heat level
# print(heat_level_counts_sorted)
```

```{python}
# This code takes a long time to run

# Dimension 2: Satisfaction

# Reference for weighted calculation: Conclusions of Question 1 and Question 2 in https://medium.com/@labdmitriy/exploring-airbnb-guest-reviews-in-london-682b45aba34e

# Define the weights for the weighted average
weights = {
    'review_scores_accuracy': 1.5,
    'review_scores_cleanliness': 2,
    'review_scores_checkin': 1,
    'review_scores_communication': 1,
    'review_scores_location': 0.5,
    'review_scores_value': 1
}

# Define the list of columns for scoring
score_columns = [
    'review_scores_accuracy', 
    'review_scores_cleanliness', 
    'review_scores_checkin', 
    'review_scores_communication', 
    'review_scores_location', 
    'review_scores_value'
]

# Convert scoring columns to float
for col in score_columns:
    data_2[col] = pd.to_numeric(data_2[col], errors='coerce')  # Set unconvertible values to NaN

# Function to calculate the weighted average score
def weighted_average(row):
    total_score = sum(row[col] * weights[col] for col in score_columns if not pd.isna(row[col]))
    total_weight = sum(weights[col] for col in score_columns if not pd.isna(row[col]))
    return total_score / total_weight if total_weight > 0 else None

# Apply the function to calculate weighted average score
data_2['weighted_score'] = data_2.apply(weighted_average, axis=1)
sorted_data_descending = data_2.sort_values(by='weighted_score', ascending=False)

# Define the function for satisfaction levels
def satisfaction_level(weighted_score):
    if weighted_score >= sorted_data_descending.weighted_score.quantile(0.80) :
        return 5
    elif weighted_score >= sorted_data_descending.weighted_score.quantile(0.60):
        return 4
    elif weighted_score >= sorted_data_descending.weighted_score.quantile(0.40):
        return 3
    elif weighted_score >= sorted_data_descending.weighted_score.quantile(0.20):
        return 2
    else:
        return 1

# Apply the function to calculate satisfaction levels
data_2['satisfaction_level'] = data_2['weighted_score'].apply(satisfaction_level)
```

```{python}
# # Show all satisfaction levels
# print(data_2['satisfaction_level'].tolist())
# print(data_2['satisfaction_level'].value_counts())
```

```{python}
# corrected_classified_file_name = 'clean_listings_2.csv'
# data_2.to_csv(corrected_classified_file_name, index=False)

# # Print the file save path
# print(f"Corrected classified listings file saved as: {corrected_classified_file_name}")
```

```{python}
# Switch
```

```{python}
# Loading the GeoPackage file for the London region
gdf_boroughs_new = gpd.read_file('Boroughs.gpkg', layer='boroughs')
geometry = [Point(xy) for xy in zip(data_2['longitude'], data_2['latitude'])]
gdf_points_new = gpd.GeoDataFrame(data_2, geometry=geometry)

# set CRS
gdf_points_new.crs = "EPSG:4326"
gdf_points_new = gdf_points_new.to_crs(gdf_boroughs_new.crs)
```

```{python}
geometry = [Point(xy) for xy in zip(data_2['longitude'], data_2['latitude'])]
gdf_quality = gpd.GeoDataFrame(data_2, geometry=geometry)

# set CRS
gdf_quality.crs = "EPSG:4326"
gdf_quality = gdf_quality.to_crs(gdf_boroughs_new.crs)
```

```{python}
# Create a canvas with two subplots
fig, axs = plt.subplots(1, 2, figsize=(20, 10))

# Adjust the spacing between subplots to make room for the legend
plt.subplots_adjust(wspace=0.3)

# First subplot: Heat levels
gdf_boroughs_new.plot(ax=axs[0], color='white', edgecolor='lightgrey')
scatter1 = gdf_points_new.plot(ax=axs[0], column='heat_level', cmap='viridis', markersize=3)

# Second subplot: Satisfaction levels
gdf_boroughs_new.plot(ax=axs[1], color='white', edgecolor='lightgrey')
scatter2 = gdf_quality.plot(ax=axs[1], column='satisfaction_level', cmap='viridis', markersize=3)

# Create a ScalarMappable object for the legend
norm = colors.Normalize(vmin=gdf_points_new['heat_level'].min(), vmax=gdf_points_new['heat_level'].max())
sm1 = plt.cm.ScalarMappable(cmap='viridis', norm=norm)
sm1.set_array([])

# Add a legend for the first subplot
divider = make_axes_locatable(axs[0])
cax = divider.append_axes("right", size="5%", pad=0.1)
fig.colorbar(sm1, cax=cax)

# Repeat the same steps for the second legend
norm = colors.Normalize(vmin=gdf_quality['satisfaction_level'].min(), vmax=gdf_quality['satisfaction_level'].max())
sm2 = plt.cm.ScalarMappable(cmap='viridis', norm=norm)
sm2.set_array([])

divider = make_axes_locatable(axs[1])
cax = divider.append_axes("right", size="5%", pad=0.1)
fig.colorbar(sm2, cax=cax)

# Set titles
axs[0].set_title('(a) Heat Levels in London')
axs[1].set_title('(b) Satisfaction Levels in London')

# Adjust the display range to only show the distribution area of the points
xmin, ymin, xmax, ymax = gdf_points_new.total_bounds
axs[0].set_xlim([xmin, xmax])
axs[0].set_ylim([ymin, ymax])

xmin, ymin, xmax, ymax = gdf_quality.total_bounds
axs[1].set_xlim([xmin, xmax])
axs[1].set_ylim([ymin, ymax])

# Show the result
plt.show()
```

```{python}
# Filter out points that meet the conditions
red_points = gdf_points_new[(gdf_points_new['heat_level'] == 5) & (gdf_points_new['satisfaction_level'] == 1)]
red_points_all = gdf_points_new[(gdf_points_new['heat_level'] == 5)]
cyan_points = gdf_points_new[(gdf_points_new['heat_level'] == 1) & (gdf_points_new['satisfaction_level'] == 5)]
cyan_points_all = gdf_points_new[(gdf_points_new['satisfaction_level'] == 5)]

# Create the figure and subplots
fig, axs = plt.subplots(1, 2, figsize=(20, 10), sharex=True, sharey=True)

# Plot the boroughs as the base layer for both subplots
gdf_boroughs.plot(ax=axs[0], color='white', edgecolor='lightgrey')
gdf_boroughs.plot(ax=axs[1], color='white', edgecolor='lightgrey')

# Plot the red and cyan points on their respective subplots
red_points_all.plot(ax=axs[0], color='mistyrose', markersize=3)
red_points.plot(ax=axs[0], color='red', markersize=3)
cyan_points_all.plot(ax=axs[1], color='lightblue', markersize=3)
cyan_points.plot(ax=axs[1], color='dodgerblue', markersize=3)

# Set the titles for both subplots
axs[0].set_title('(c) "Heat5" with highlighted "Satisfaction1"')
axs[1].set_title('(d) "Heat1" with highlighted "Satisfaction5"')

# Determine the combined bounds of both datasets
xmin = min(red_points.total_bounds[0], cyan_points.total_bounds[0])
ymin = min(red_points.total_bounds[1], cyan_points.total_bounds[1])
xmax = max(red_points.total_bounds[2], cyan_points.total_bounds[2])
ymax = max(red_points.total_bounds[3], cyan_points.total_bounds[3])

# Adjust the display range to display only the distribution area of ​​the points
xmin, ymin, xmax, ymax = gdf_points_new.total_bounds
axs[0].set_xlim([xmin, xmax])
axs[0].set_ylim([ymin, ymax])

xmin, ymin, xmax, ymax = gdf_quality.total_bounds
axs[1].set_xlim([xmin, xmax])
axs[1].set_ylim([ymin, ymax])

# Ensure the aspect ratio is the same for both subplots
axs[0].set_aspect('equal', adjustable='box')
axs[1].set_aspect('equal', adjustable='box')

# Show the plots
plt.show()
```

```{python}
# red_points['price'] = red_points['price'].replace('[\$,]', '', regex=True).astype(float)
# print(red_points['price'].describe())

# red_points_all['price'] = red_points_all['price'].replace('[\$,]', '', regex=True).astype(float)
# print(red_points_all['price'].describe())

# cyan_points['price'] = cyan_points['price'].replace('[\$,]', '', regex=True).astype(float)
# print(cyan_points['price'].describe())

# cyan_points_all['price'] = cyan_points_all['price'].replace('[\$,]', '', regex=True).astype(float)
# print(cyan_points_all['price'].describe())
```

```{python}
# red_points = gdf_points_new[(gdf_points_new['heat_level'] == 5) & (gdf_points_new['satisfaction_level'] == 1)]
# cyan_points = gdf_points_new[(gdf_points_new['heat_level'] == 1) & (gdf_points_new['satisfaction_level'] == 5)]

# fig, ax = plt.subplots(figsize=(10, 10))

# gdf_boroughs.plot(ax=ax, color='white', edgecolor='lightgrey')
# red_points.plot(ax=ax, color='red', markersize=10, label='Heat: 5, Quality: 1')
# cyan_points.plot(ax=ax, color='cyan', markersize=10, label='Heat: 1, Quality: 5')

# ax.set_title('Special Points in London')

# xmin, ymin, xmax, ymax = gdf_points_new.total_bounds
# ax.set_xlim([xmin, xmax])
# ax.set_ylim([ymin, ymax])

# ax.legend()

# plt.show()
```

```{python}
# print(gdf_quality['satisfaction_level'].describe())
# print(gdf_points_new['heat_level'].describe())
```

```{python}
# Switch
```

```{python}
# data_2.info()
```

```{python}
# # ！！！！！！Since the running time of the subsequent code is too long, comment out the unnecessary parts here. If it needs to be run, it will take more than ten minutes.

# from sklearn.preprocessing import OneHotEncoder
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.feature_extraction.text import CountVectorizer
# from sklearn.decomposition import LatentDirichletAllocation
```

```{python}
# import nltk
# import spacy
# from nltk.corpus import wordnet as wn
# from nltk.stem.wordnet import WordNetLemmatizer
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize, sent_tokenize
# from nltk.tokenize.toktok import ToktokTokenizer
# from nltk.stem.porter import PorterStemmer
# from nltk.stem.snowball import SnowballStemmer
# from nltk import ngrams, FreqDist

# lemmatizer = WordNetLemmatizer()
# tokenizer = ToktokTokenizer()
```

```{python}
# # the method in the practical-07
# import urllib.request
# host  = 'https://orca.casa.ucl.ac.uk'
# turl  = f'{host}/~jreades/__textual__.py'
# tdirs = os.path.join('textual')
# tpath = os.path.join(tdirs,'__init__.py')

# if not os.path.exists(tpath):
#     os.makedirs(tdirs, exist_ok=True)
#     urllib.request.urlretrieve(turl, tpath)
```

```{python}
# from textual import *;
```

```{python}
# # select the houses which is 5 in heat_level and satisfaction_level
# heat5_house = data_2[data_2['heat_level'] == 5]
# satisfaction5_house = data_2[data_2['satisfaction_level'] == 5]
# analyzeheat5 = heat5_house

# # select the houses which is 4 in heat_level and satisfaction_level
# heat4_house = data_2[data_2['heat_level'] == 4]
# satisfaction4_house = data_2[data_2['satisfaction_level'] == 4]
# analyzeheat4 = heat4_house

# # select the houses which is 3 in heat_level and satisfaction_level
# heat3_house = data_2[data_2['heat_level'] == 3]
# satisfaction3_house = data_2[data_2['satisfaction_level'] == 3]
# analyzeheat3 = heat3_house

# # select the houses which is 2 in heat_level and satisfaction_level
# heat2_house = data_2[data_2['heat_level'] == 2]
# satisfaction2_house = data_2[data_2['satisfaction_level'] == 2]
# analyzeheat2 = heat2_house

# # select the houses which is 1 in heat_level and satisfaction_level
# heat1_house = data_2[data_2['heat_level'] == 1]
# satisfaction1_house = data_2[data_2['satisfaction_level'] == 1]
# analyzeheat1 = heat1_house

# highHeat_lowSat = data_2[(data_2['satisfaction_level']) == 1 & (data_2['heat_level'] == 5)]
# lowHeat_highSat = data_2[(data_2['satisfaction_level'])== 5 & (data_2['heat_level'] == 1)]
```

```{python}
# heatAll = [heat1_house,heat2_house,heat3_house,heat4_house,heat5_house]
# satisfactionAll = [satisfaction1_house,satisfaction2_house,satisfaction3_house,satisfaction4_house,satisfaction5_house]
# extreme = [highHeat_lowSat, lowHeat_highSat]
```

```{python}
# # It takes a long time from this code block to the last cell to appear.

# # Calculate the average of host_response_rate
# host_response_rate = []
# for i in heatAll:
#     j = i.dropna(subset=['host_response_rate'])
#     mean = j['host_response_rate'].str.rstrip('%').astype('float') / 100.0
#     host_response_rate.append(mean.mean())

# for i in satisfactionAll:
#     j = i.dropna(subset=['host_response_rate'])
#     mean = j['host_response_rate'].str.rstrip('%').astype('float') / 100.0
#     host_response_rate.append(mean.mean())
    
# for i in extreme:
#     j = i.dropna(subset=['host_response_rate'])
#     mean = j['host_response_rate'].str.rstrip('%').astype('float') / 100.0
#     host_response_rate.append(mean.mean())
```

```{python}
# # Calculate the average of host_acceptance_rate
# host_acceptance_rate = []
# for i in heatAll:
#     j = i.dropna(subset=['host_acceptance_rate'])
#     mean = j['host_acceptance_rate'].str.rstrip('%').astype('float') / 100.0
#     host_acceptance_rate.append(mean.mean())

# for i in satisfactionAll:
#     j = i.dropna(subset=['host_acceptance_rate'])
#     mean = j['host_acceptance_rate'].str.rstrip('%').astype('float') / 100.0
#     host_acceptance_rate.append(mean.mean())

# for i in extreme:
#     j = i.dropna(subset=['host_acceptance_rate'])
#     mean = j['host_acceptance_rate'].str.rstrip('%').astype('float') / 100.0
#     host_acceptance_rate.append(mean.mean())
```

```{python}
# # Calculate the average length of neighborhood_overview text
# neighborhood_overview = []
# for i in heatAll:
#     j = i.dropna(subset=['neighborhood_overview'])
#     j['neighborhood_overview'] = j.neighborhood_overview.apply(normalise_document, remove_digits=True)
#     textlist = j.neighborhood_overview.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     neighborhood_overview.append(averge)

# for i in satisfactionAll:
#     j = i.dropna(subset=['neighborhood_overview'])
#     j['neighborhood_overview'] = j.neighborhood_overview.apply(normalise_document, remove_digits=True)
#     textlist = j.neighborhood_overview.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     neighborhood_overview.append(averge)

# for i in extreme:
#     j = i.dropna(subset=['neighborhood_overview'])
#     j['neighborhood_overview'] = j.neighborhood_overview.apply(normalise_document, remove_digits=True)
#     textlist = j.neighborhood_overview.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     neighborhood_overview.append(averge)
```

```{python}
# # Calculate the average length of description text
# description = []
# for i in heatAll:
#     j = i.dropna(subset=['description'])
#     j['description'] = j.description.apply(normalise_document, remove_digits=True)
#     textlist = j.description.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     description.append(averge)

# for i in satisfactionAll:
#     j = i.dropna(subset=['description'])
#     j['description'] = j.description.apply(normalise_document, remove_digits=True)
#     textlist = j.description.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     description.append(averge)

# for i in extreme:
#     j = i.dropna(subset=['description'])
#     j['description'] = j.description.apply(normalise_document, remove_digits=True)
#     textlist = j.description.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     description.append(averge)
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
# # Calculate the average length of host_about text
# host_about = []
# for i in heatAll:
#     j = i.dropna(subset=['host_about'])
#     j['host_about'] = j.host_about.apply(normalise_document, remove_digits=True)
#     textlist = j.host_about.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     host_about.append(averge)

# for i in satisfactionAll:
#     j = i.dropna(subset=['host_about'])
#     j['host_about'] = j.host_about.apply(normalise_document, remove_digits=True)
#     textlist = j.host_about.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     host_about.append(averge)

# for i in extreme:
#     j = i.dropna(subset=['host_about'])
#     j['host_about'] = j.host_about.apply(normalise_document, remove_digits=True)
#     textlist = j.host_about.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     host_about.append(averge)
```

```{python}
# # Calculate the average of price
# price= []
# for i in heatAll:
#     i['price'] = i['price'].str.replace('$','', regex=False).str.replace(',','', regex=False).astype('float')
#     mean =  i.price.mean()
#     price.append(mean)

# for i in satisfactionAll:
#     i['price'] = i['price'].str.replace('$','', regex=False).str.replace(',','', regex=False).astype('float')
#     mean =  i.price.mean()
#     price.append(mean)

# for i in extreme:
#     i['price'] = i['price'].str.replace('$','', regex=False).str.replace(',','', regex=False).astype('float')
#     mean =  i.price.mean()
#     price.append(mean)
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
# # Creating df table
# row_list = ['heat1','heat2','heat3','heat4','heat5',
#             'sat1','sat2','sat3','sat4','sat5',
#             'heat5_sat1', 'heat1_sat5']

# dir1 = { 'Level': row_list,
#         'price(£)': price,
#         'host_response_rate(%)': host_response_rate,
#         'host_acceptance_rate(%)': host_acceptance_rate
#       }

# dir2 = { 'Level': row_list,
#         'neighborhood_overview': neighborhood_overview,
#         'description': description,
#         'host_about': host_about
#       }
```

```{python}
# # format DataFrame1
# formatted_df1 = pd.DataFrame(dir1)

# # Format columns
# formatted_df1['price(£)'] = formatted_df1['price(£)'].round(2)

# # Convert decimal to percent with two decimal places
# formatted_df1['host_response_rate(%)'] = (formatted_df1['host_response_rate(%)'] * 100).round(2)
# formatted_df1['host_acceptance_rate(%)'] = (formatted_df1['host_acceptance_rate(%)'] * 100).round(2)

# # format DataFrame2
# formatted_df2 = pd.DataFrame(dir2)

# # Convert column to integer
# formatted_df2['neighborhood_overview'] = formatted_df2['neighborhood_overview'].astype(int)
# formatted_df2['description'] = formatted_df2['description'].astype(int)
# formatted_df2['host_about'] = formatted_df2['host_about'].astype(int)
```

```{python}
# formatted_df1.to_csv('formatted_df1.csv', index=False)
# formatted_df2.to_csv('formatted_df2.csv', index=False)
```

```{python}
df_savetime1 = pd.read_csv('formatted_df1.csv')
df_savetime2 = pd.read_csv('formatted_df2.csv')
```

```{python}
# # Set up Styler to format tables
# df_styled1 = df_savetime1.style.format({
#     'price(£)': "{:.2f}",
#     'host_response_rate(%)': "{:.2f}",
#     'host_acceptance_rate(%)': "{:.2f}",
# }).set_properties(**{'text-align': 'center'})

# df_styled1.hide(axis='index')
```

```{python}
# # Set up Styler to format tables
# df_styled2 = df_savetime2.style.format({
#     'neighborhood_overview': "{:.0f}",
#     'description': "{:.0f}",
#     'host_about': "{:.0f}"
# }).set_properties(**{'text-align': 'center'})

# df_styled2.hide(axis='index')
```

```{python}
df1 = df_savetime1.copy()
df1.set_index('Level', inplace=True)

# Create a bar chart
ax = df1[['price(£)']].plot(kind='bar', figsize=(14, 6), color=['#a8dadc'])

# Set title and labels
ax.set_title('(a) Price', fontsize=16)
ax.set_xlabel('Level', fontsize=12)
ax.set_ylabel('Count', fontsize=12)

# Set the x-axis labels to be tilted at 45 degrees
plt.xticks(rotation=45)

# Set the range of the y-axis
ax.set_ylim(0, 400)

# Display the legend
ax.legend(title='Legend')

# Show the plot
plt.tight_layout()
plt.show()
```

```{python}
df2 = df_savetime1.copy()
df2.set_index('Level', inplace=True)

# Create a bar chart
ax = df2[['host_response_rate(%)', 'host_acceptance_rate(%)']].plot(kind='bar', figsize=(14, 6), color=['#0077b6', '#90e0ef'])

# Set title and labels
ax.set_title('(b) Response rate and acceptance rate', fontsize=16)
ax.set_xlabel('Level', fontsize=12)
ax.set_ylabel('Count', fontsize=12)

# Set the x-axis labels to be tilted at 45 degrees
plt.xticks(rotation=45)

# Set the range of the y-axis
ax.set_ylim(70, 120)

# Display the legend
ax.legend(title='Legend')

# Show the plot
plt.tight_layout()
plt.show()
```

```{python}
df3 = df_savetime2.copy()
df3.set_index('Level', inplace=True)

# Create a bar chart
ax = df3.plot(kind='bar', figsize=(14, 6), color=['#023e8a', '#00b4d8', '#ade8f4'])

# Set title and labels
ax.set_title('(c) The length of three types of descriptive text', fontsize=16)
ax.set_xlabel('Level', fontsize=12)
ax.set_ylabel('Count', fontsize=12)

# Set the x-axis labels to be tilted at 45 degrees
plt.xticks(rotation=45)

# Set the range of the y-axis
ax.set_ylim(0, 650)

# Display the legend
ax.legend(title='Legend')

# Show the plot
plt.tight_layout()
plt.show()
```

## 6. With reference to the data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London? 



## 7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how *could* this data set be used to inform the regulation of Short-Term Lets (STL) in London? 



## References
