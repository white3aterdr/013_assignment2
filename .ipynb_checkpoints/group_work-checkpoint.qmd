---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Rookie Programmers Group Project
nocite: '@*'
execute:
  echo: false
format:
  html:
    theme:
      - minty
      - css/web.scss
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto
    monofont: JetBrainsMono-Regular
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Declaration of Authorship {.unnumbered .unlisted}

We, Rookie Programmers, confirm that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date: 2023/12/11

Student Numbers: 23148316 / 23100260 / 23049577 / 23202303

Word count: 2490

## Brief Group Reflection

| What Went Well | What Was Challenging |
| -------------- | -------------------- |
| Great collaboration and division of workload within the group. | It was difficult to select topics and find other data sets. |
| The pace of work is good and it is basically completed in week 9. | Our original plan did not lead to the expected conclusion. |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

We attempted to develop two rating criteria in our analysis, which required defining weights. But there are few references. In the urban planning industry, when similar situations are involved, definitions are sometimes defined by inviting experts to score. So for group work like ours, can we first find reasonable indicators based on current conditions, and adopt similar methods like experts score to optimize in the future if necessary and conditional?

In our analysis, we initially wanted to use NLP to analyze subjective descriptive text. However, in practice, we found that only using keyword frequency analysis cannot draw effective conclusions, and the skills we have mastered are not enough to complete it. Although we have adjusted the research direction later, what should we do if in some cases, a research does not reach a conclusion of obvious significance in the end?

```{=html}
<style type="text/css">
.duedate {
  border: dotted 2px red; 
  background-color: rgb(255, 235, 235);
  height: 50px;
  line-height: 50px;
  margin-left: 40px;
  margin-right: 40px
  margin-top: 10px;
  margin-bottom: 10px;
  color: rgb(150,100,100);
  text-align: center;
}
</style>
```

{{< pagebreak >}}

# Response to Questions

```{python}
import os
import requests
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as colors
import seaborn as sns
import geopandas as gpd
from shapely.geometry import Point
from mpl_toolkits.axes_grid1 import make_axes_locatable
```

```{python}
import warnings
warnings.filterwarnings('ignore')
```

```{python}
# Check if the data file exists, if not, download it
file_gz = 'listings.csv.gz'
url_gz = 'https://github.com/white3aterdr/013_assignment2/raw/main/listings.csv.gz'

if os.path.exists(file_gz):
    data = pd.read_csv(file_gz, compression='gzip', encoding='ISO-8859-1', dtype={68: str}, low_memory=False)
else: 
    data = pd.read_csv(url_gz, compression='gzip', encoding='ISO-8859-1', dtype={68: str}, low_memory=False)
    data.to_csv(file_gz)

# Check if the bibliography file exists, if not, download it
file_bib = "bio.bib"
url_bib = "https://github.com/white3aterdr/013_assignment2/raw/main/bio.bib"

if not os.path.exists(file_bib):
    response = requests.get(url_bib)
    with open(file_bib, "wb") as file:
        file.write(response.content)

# Check if the style file exists, if not, download it
file_csl = "harvard-cite-them-right.csl"
url_csl = "https://github.com/white3aterdr/013_assignment2/raw/main/harvard-cite-them-right.csl"

if not os.path.exists(file_csl):
    response = requests.get(url_csl)
    with open(file_csl, "wb") as file:
        file.write(response.content)

# Check if the geographic file exists, if not, download it
file_gpkg = "Boroughs.gpkg"
url_gpkg = "https://github.com/white3aterdr/013_assignment2/raw/main/Boroughs.gpkg"

if not os.path.exists(file_gpkg):
    response = requests.get(url_gpkg)
    with open(file_gpkg, "wb") as file:
        file.write(response.content)

# Check if the csv file exists, if not, download it
file1_csv = "formatted_df1.csv"
url1_csv = "https://github.com/white3aterdr/013_assignment2/raw/main/formatted_df1.csv"

if not os.path.exists(file1_csv):
    response = requests.get(url1_csv)
    with open(file1_csv, "wb") as file:
        file.write(response.content)

# Check if the csv file exists, if not, download it
file2_csv = "formatted_df2.csv"
url2_csv = "https://github.com/white3aterdr/013_assignment2/raw/main/formatted_df2.csv"

if not os.path.exists(file2_csv):
    response = requests.get(url2_csv)
    with open(file2_csv, "wb") as file:
        file.write(response.content)
```

## 1. Who collected the data?

Insideairbnb website data[@noauthor_home_nodate] were collected by the Inside Airbnb project, with contributions from collaborators and partners.

The raw data were compiled from the Airbnb[@noauthor_airbnb_nodate-1], which was collected by the company Airbnb, Inc.


## 2. Why did they collect it?

On the insideairbnb website [@noauthor_about_nodate], they write:

>"We work towards a vision where communities are empowered with data and information to understand, decide and control the role of renting residential homes to tourists".

As for the company Airbnb, three considerations may arise:
  
- Operational aspect: To assist users in making informed decisions.  

- Company aspect: Data collection plays a crucial role in analyzing Airbnb's operational performance. With the gathered data, Airbnb can fine-tune its operational strategies.

- Legal aspect: In compliance with diverse regulatory requirements, Airbnb must furnish information to the respective authorities as necessary.


## 3. How was the data collected?  Insideairbnb collected data, which was a snapshot at a particular time, from Airbnb platform.

Insideairbnb collected data, which was a snapshot at a particular time, from Airbnb platform.

The data collected by Airbnb can be classified along two dimensions: the data source and the method of generation.


#### From host:

- Objective data. From hosts but can be verified by images or qualifications. (Listing’s: latitude, property_type, etc. Host’s: host_name, host_location, etc.)

- Produced during platform operation. Platform generates these data, but host behaviour determines their content. (Host_acceptance_rate, host_response_rate, etc.)

- Subjective data. These subjective descriptions were provided by the host. Its content and tendency reflect the host's preferences. (Description, neighborhood_overview, host_about, etc.)

#### From guest: 

- Objective data. When guests register an Airbnb account, they need to provide personal information. (Name, contact_details, payment_information, etc.)

- Generated during platform operation. The platform created these data, but guest behaviour determines their content. (Number_of_reviews, first_review, etc.)

- Subjective data. Based on guest’s preference. (Review_scores_accuracs, comments, etc.)

#### From platform: 

- Generated during platform operation. These data were automatically generated and user's behavior has no impact on it. (Host_id, host_url, reviewer_id, etc.)

- Platform does not proactively provide objective or subjective data.


## 4. How does the method of collection impact the completeness and/or accuracy of its representation of the process it seeks to study, and what wider issues does this raise?

#### Completeness:

- Insideairbnb data is a snapshot of the Airbnb platform, it cannot reflect changing information.

- Regarding the original data on Airbnb, if the collection relies on voluntary submissions (reviews from guests), it might not be possible to gather data from all relevant individuals. Self-selection bias could lead to incomplete data.

#### Accuracy: 

- Objective data is mostly accurate, but Airbnb anonymizes listing location. The location on the map will be offset by 0-150 meters. Airbnb anonymizes listings in the same building, making them appear "fragmented"[@noauthor_data_nodate].

- Data generated by the platform might be accurate but can also be subject to technical errors. Data tampering, such as fake review or inflated booking figures, may introduce deceptive listing evaluation.

- The Accuracy and completeness of subjective information depend on the provider's skill and desire to deliver precise details. Even for listing descriptions, hosts with diverse professional experience may present them differently. Hosts with less expertise may overlook benefits or guest concerns. However, expert landlords can highlight desirable characteristics, while deceitful landlords may hide faults. Guests have varying histories and judgement criteria, therefore listing ratings are more subjective. One user may overlook a detail that another values, skewing the evaluation.


## 5. What ethical considerations does the use of this data raise? 

#### Privacy and Security: 

- Privacy Concerns: Individual data, including names, contact information, and payment details, is sensitive. The collection, storage, and use of this data raises privacy concerns. Users should be informed about what data is collected and how it will be used and give consent, especially for identifying data. News sources say some landlords dislike the platform's photo demand.

- Data Security and Usage Control: Preventing unauthorised access and breaches is essential. Strong security measures to protect user data from cyberattacks are ethical. It's also crucial to avoid misusing data obtained for one purpose without user consent. Avoid disclosing or using data for unknown purposes.

#### Bias and Discrimination: 

- The data collection and analysis may inadvertently create bias or overlook certain groups. Such as the feelings of groups lack willingness to comment.

- Data collected from user reviews or host descriptions may reflect personal biases. This could lead to racial, gender, age, and other prejudice. Identifying and mitigating biases is essential for ethical data use.

- The Matthew effect brought about by the popularity mechanism, such as Super_hosts. This inequality may result in popular listings being in short supply, while unpopular listings remain uninterested.

#### Distortion and Misleading: 

- Host capabilities may cause photo or description distortions, misleading listing status. This mismatch can make it hard for guests to assess the listing's quality.

- Fake photographs and reviews can deceive users during intentional deception. This can also lead to inaccurate data analysis. The platform must build up detection, elimination, and punishment systems.

#### Impact on Communities: 

- When utilizing data for descriptions of communities, the produced data only offer a partial reflection of the status of certain communities, not encompassing all aspects. However, the release of these findings can have diverse impacts on the community, including external evaluations, rent fluctuations, effects on residents' lives, and socioeconomic stratification.


## 6. With reference to the data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London? 

In the analysis of the previous question, we noticed one of the peculiarities of insideairbnb  data is the enormous amount of listings. On traditional platforms, the number of guests is usually on the order of magnitude of several for each business, while on Airbnb the situation is different.

In addition, for each listing, hosts need to provide a lot of subjective descriptive information. Obviously, both the issues of completeness and accuracy mentioned earlier can affect the order volume. The gap between specialized hosts and general hosts in this regard may be even greater.

This impact also leads to a disparity between the description of listings and their actual conditions. When booking on Airbnb, users pay attention to the rating, and listings with higher ratings should logically be more popular. However, "popularity" and "satisfaction" are two different standards in fact. There are houses with high ratings but less-than-ideal booking data, while other houses with average quality have high occupancy rates.

We hope to take London as an example and, through the study of listings, find patterns of features to improve listings with high satisfaction but low popularity, making them as popular as they deserve. This is fairer to hosts who carefully manage their properties and help guests efficiently identify good listings.

Before the analysis, it needs to be clarified that the impact of location on popularity is also obvious. Considering that most of London's listings serve tourists, we focus on the areas with the highest density of attractions. Figure 1-a shows listings with reviews still available after 2019, about 88,000, and a high central density can be observed. Figure 1-b represents the core boroughs of London, with a reduced range but still about 60,000 listings. According to the distribution of London attractions, the differences in transportation convenience in these areas are not significant, minimizing the impact of location in the subsequent analysis.

```{python}
# data.shape
```

```{python}
# data.info()
```

```{python}
# Data cleaning step1: Check the number of missing values ​​in each column and sort them in descending order
data.loc[:, data.isnull().sum() > 0].isnull().sum().sort_values(ascending=False);
```

```{python}
# Data cleaning step 2: Check the columns whose content is 0 or basically unchanged
data.loc[:, data.nunique() <= 1].nunique().sort_values();
```

```{python}
# Clean the data based on the above results and delete unnecessary columns
data = data.drop(columns=['scrape_id', 'calendar_updated', 'neighbourhood_group_cleansed', 'license', 'bathrooms'])
```

```{python}
# Make sure the 'last_review' column is in datetime format
data['last_review'] = pd.to_datetime(data['last_review'], errors='coerce')

# Filter out rows in 'last_review' with years less than 2019
data = data[data['last_review'].dt.year >= 2019]
```

```{python}
# data.info()
```

```{python}
# This code takes a long time to run

# Read the GeoPackage file for London boroughs
gdf_boroughs = gpd.read_file('Boroughs.gpkg', layer='boroughs')

# Create geographical data for points
geometry = [Point(xy) for xy in zip(data['longitude'], data['latitude'])]
gdf_points = gpd.GeoDataFrame(data, geometry=geometry)

# Ensure the CRS of the points is the same as the map's CRS
gdf_points.crs = "EPSG:4326"
data = gdf_points.to_crs(gdf_boroughs.crs)

# Boroughs to be retained
selected_boroughs = [
    'Camden', 
    'Hackney', 
    'Hammersmith and Fulham', 
    'Islington', 
    'Kensington and Chelsea', 
    'Lambeth', 
    'Lewisham', 
    'Southwark', 
    'Tower Hamlets', 
    'Wandsworth', 
    'Westminster',
    'City of London'
]

# Create a Boolean series indicating whether each point is within the selected boroughs
in_selected_borough = data.apply(lambda x: any(borough.contains(x.geometry) for borough in gdf_boroughs[gdf_boroughs['NAME'].isin(selected_boroughs)].geometry), axis=1)

# Create two side-by-side maps
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), facecolor='white') 

# Plot the left map: Unfiltered data
gdf_boroughs.plot(ax=ax1, color='white', edgecolor='grey', linewidth=0.6)
data.plot(ax=ax1, color='deepskyblue', markersize=1, alpha=0.5)
ax1.axis('off')

# Plot the right map: Filtered data
gdf_boroughs.plot(ax=ax2, color='white', edgecolor='grey', linewidth=0.6)
data[in_selected_borough].plot(ax=ax2, color='deepskyblue', markersize=0.4, alpha=0.3)
ax2.axis('off')

# Set titles
ax1.set_title('Figure 1-a  All Points')
ax2.set_title('Figure 1-b  Points in Selected Boroughs')

# Show the map
plt.show()
```

For listings in this area, we have established two sets of rating standards for "popularity" and "satisfaction" to discover their distribution patterns.

- Popularity = 1 * 30_day_booking_rate + 10 * average_daily_review_count + 0.5 * superhost_or_not


The availability within 30 days usually reflects the number of nights not booked and can calculate the 30-day booking rate. However, listings with availability of 0 within 60 days need to be excluded because the longer unavailable days are usually set by the host as non-bookable. The average daily review count can be calculated based on the number of reviews and the the days between the first and last reviews. A superhost is an identification by the platform for high-popularity hosts. We set the weighting coefficient based on the concept of review rate, combined with insideairbnb's assumptions [@noauthor_about_nodate]. Afterward, for listings that meet the conditions, they are evenly divided into five levels according to the score, with level 1 having the lowest popularity score and level 5 having the highest.

- Satisfaction = 1.5 * accuracy + 2 * cleanliness + 1 * check in + 1 * communication + 0.5 * location + 1 * value

Satisfaction weight refers to [@labazkin_exploring_2019], and we use the overall score and item scores to be reweighted, giving higher weight to the items reflecting the listing itself. Similarly, a classification of 1-5 levels is carried out.

The number of filtered and classified listings is 23,114, and their distribution in the core area of London is shown in Figure 2. Figure 2-a shows a high overlap of high-popularity areas with tourist hotspots, such as Hyde Park, Regent's Park, and the surrounding area of the Tower Bridge. Figure 2-b shows that the area with the lowest satisfaction also appears here. In other areas, popularity and satisfaction are more randomly distributed. Figure 2-c highlights the 4,622 listings with a popularity of 5, among which 774 have a satisfaction of 1, showing clustering in certain locations. Figure 2-d highlights the 4,623 listings with a satisfaction of 5, among which 776 have a popularity of 1, indicating that even in popular locations, there are many listings with low popularity but high satsfaction.

```{python}
# Change the variable name here to avoid subsequent changes to the code that will take too long to read the data from the beginning.
data_2 = data[in_selected_borough].copy()
```

```{python}
# # save csv file
# data_2.to_csv('clean_listings_1.csv', index=False)
```

```{python}
# data_2.info()
```

```{python}
# Find rows that contain missing values ​​in satisfaction ratings and popularity ratings
columns_to_check = [
    'review_scores_accuracy', 
    'review_scores_cleanliness', 
    'review_scores_checkin', 
    'review_scores_communication', 
    'review_scores_location', 
    'review_scores_value',
    'last_review',
    'first_review',
    'number_of_reviews',
    'availability_60',
    'availability_30',
]

# Delete rows with missing values ​​in these columns
data_2 = data_2.dropna(subset=columns_to_check)
```

```{python}
# Delete the columns that have no vacancies within 60 days. We believe that these listings are not fully booked, but cannot be booked due to the host settings.
data_2 = data_2[data_2['availability_60'] != 0]
```

```{python}
# Dimension 1: Popularity

# Ensure the date columns are in datetime format
data_2['last_review'] = pd.to_datetime(data_2['last_review'])
data_2['first_review'] = pd.to_datetime(data_2['first_review'])

# Calculate the difference between dates and convert to days
data_2['operating_days'] = (data_2['last_review'] - data_2['first_review']).dt.days

# To avoid division by zero, we can add a small number to the denominator, like 0.1. Here, we choose to filter out these rows instead
data_2 = data_2[data_2['operating_days'] > 0]

# Calculate reviews per day
data_2['reviews_per_day'] = data_2['number_of_reviews'] / (data_2['operating_days'])

# Remove data where reviews per day > 1
data_2 = data_2[data_2['reviews_per_day'] <= 1]
```

```{python}
# Calculate monthly occupancy rate
data_2['occupancy_rate'] = ((30 - data_2['availability_30']) / 30) 

# Add the weighted values ​​of occupancy_rate and reviews_per_day, where the number of comments is combined with the concept of review rate, and is weighted 10 times
data_2['combined_value'] = data_2['occupancy_rate'] + 10*data_2['reviews_per_day']
```

```{python}
# result_columns = ['number_of_reviews', 'operating_days', 'reviews_per_day', 'occupancy_rate', 'combined_value']
# print(data_2[result_columns].head())
```

```{python}
# Update the value of the 'reviews_per_day_super_host' column based on 'host_is_superhost'
data_2['super_host_rate'] = data_2.apply(
    lambda row: row['combined_value'] + 0.5 if row['host_is_superhost'] == 't' else row['combined_value'],
    axis=1
)
```

```{python}
# print(data_2[['host_is_superhost', 'combined_value', 'super_host_rate']].head())
```

```{python}
# data_2['reviews_per_day'].describe()
```

```{python}
# data_2['occupancy_rate'].describe()
```

```{python}
# data_2['super_host_rate'].describe()
```

```{python}
# Assume that a fixed value is used as a threshold to classify popularity. Here we need five thresholds divided into six intervals.
dimension_1 = data_2.sort_values(by='super_host_rate',ascending=False)

thresholds = [dimension_1.super_host_rate.quantile(0.00),
              dimension_1.super_host_rate.quantile(0.20), 
              dimension_1.super_host_rate.quantile(0.40), 
              dimension_1.super_host_rate.quantile(0.60), 
              dimension_1.super_host_rate.quantile(0.80), 
              dimension_1.super_host_rate.quantile(1.00)]  # Example threshold
labels = [1, 2, 3, 4, 5]  # Five  popularity tags

# Distribute popularity
data_2['heat_level'] = pd.cut(data_2['super_host_rate'], bins=thresholds, labels=labels, include_lowest=True)
```

```{python}
# Assuming the popularity has already been calculated and assigned to the 'heat_level' column
heat_level_counts = data_2['heat_level'].value_counts()

# Sort by heat level
heat_level_counts_sorted = heat_level_counts.sort_index()

# Print the count statistics for each heat level
# print(heat_level_counts_sorted)
```

```{python}
# This code takes a long time to run

# Dimension 2: Satisfaction

# Reference for weighted calculation: Conclusions of Question 1 and Question 2 in https://medium.com/@labdmitriy/exploring-airbnb-guest-reviews-in-london-682b45aba34e

# Define the weights for the weighted average
weights = {
    'review_scores_accuracy': 1.5,
    'review_scores_cleanliness': 2,
    'review_scores_checkin': 1,
    'review_scores_communication': 1,
    'review_scores_location': 0.5,
    'review_scores_value': 1
}

# Define the list of columns for scoring
score_columns = [
    'review_scores_accuracy', 
    'review_scores_cleanliness', 
    'review_scores_checkin', 
    'review_scores_communication', 
    'review_scores_location', 
    'review_scores_value'
]

# Convert scoring columns to float
for col in score_columns:
    data_2[col] = pd.to_numeric(data_2[col], errors='coerce')  # Set unconvertible values to NaN

# Function to calculate the weighted average score
def weighted_average(row):
    total_score = sum(row[col] * weights[col] for col in score_columns if not pd.isna(row[col]))
    total_weight = sum(weights[col] for col in score_columns if not pd.isna(row[col]))
    return total_score / total_weight if total_weight > 0 else None

# Apply the function to calculate weighted average score
data_2['weighted_score'] = data_2.apply(weighted_average, axis=1)
sorted_data_descending = data_2.sort_values(by='weighted_score', ascending=False)

# Define the function for satisfaction levels
def satisfaction_level(weighted_score):
    if weighted_score >= sorted_data_descending.weighted_score.quantile(0.80) :
        return 5
    elif weighted_score >= sorted_data_descending.weighted_score.quantile(0.60):
        return 4
    elif weighted_score >= sorted_data_descending.weighted_score.quantile(0.40):
        return 3
    elif weighted_score >= sorted_data_descending.weighted_score.quantile(0.20):
        return 2
    else:
        return 1

# Apply the function to calculate satisfaction levels
data_2['satisfaction_level'] = data_2['weighted_score'].apply(satisfaction_level)
```

```{python}
# # Show all satisfaction levels
# print(data_2['satisfaction_level'].tolist())
# print(data_2['satisfaction_level'].value_counts())
```

```{python}
# corrected_classified_file_name = 'clean_listings_2.csv'
# data_2.to_csv(corrected_classified_file_name, index=False)

# # Print the file save path
# print(f"Corrected classified listings file saved as: {corrected_classified_file_name}")
```

```{python}
# Switch
```

```{python}
# Loading the GeoPackage file for the London region
gdf_boroughs_new = gpd.read_file('Boroughs.gpkg', layer='boroughs')
geometry = [Point(xy) for xy in zip(data_2['longitude'], data_2['latitude'])]
gdf_points_new = gpd.GeoDataFrame(data_2, geometry=geometry)

# set CRS
gdf_points_new.crs = "EPSG:4326"
gdf_points_new = gdf_points_new.to_crs(gdf_boroughs_new.crs)
```

```{python}
geometry = [Point(xy) for xy in zip(data_2['longitude'], data_2['latitude'])]
gdf_quality = gpd.GeoDataFrame(data_2, geometry=geometry)

# set CRS
gdf_quality.crs = "EPSG:4326"
gdf_quality = gdf_quality.to_crs(gdf_boroughs_new.crs)
```

```{python}
# Create a canvas with two subplots
fig, axs = plt.subplots(1, 2, figsize=(20, 10))

# Adjust the spacing between subplots to make room for the legend
plt.subplots_adjust(wspace=0.3)

# First subplot: Heat levels
gdf_boroughs_new.plot(ax=axs[0], color='white', edgecolor='lightgrey')
scatter1 = gdf_points_new.plot(ax=axs[0], column='heat_level', cmap='viridis', markersize=3)

# Second subplot: Satisfaction levels
gdf_boroughs_new.plot(ax=axs[1], color='white', edgecolor='lightgrey')
scatter2 = gdf_quality.plot(ax=axs[1], column='satisfaction_level', cmap='viridis', markersize=3)

# Create a ScalarMappable object for the legend
norm = colors.Normalize(vmin=gdf_points_new['heat_level'].min(), vmax=gdf_points_new['heat_level'].max())
sm1 = plt.cm.ScalarMappable(cmap='viridis', norm=norm)
sm1.set_array([])

# Add a legend for the first subplot
divider = make_axes_locatable(axs[0])
cax = divider.append_axes("right", size="5%", pad=0.1)
fig.colorbar(sm1, cax=cax)

# Repeat the same steps for the second legend
norm = colors.Normalize(vmin=gdf_quality['satisfaction_level'].min(), vmax=gdf_quality['satisfaction_level'].max())
sm2 = plt.cm.ScalarMappable(cmap='viridis', norm=norm)
sm2.set_array([])

divider = make_axes_locatable(axs[1])
cax = divider.append_axes("right", size="5%", pad=0.1)
fig.colorbar(sm2, cax=cax)

# Set titles
axs[0].set_title('Figure 2-a  Heat Levels in London')
axs[1].set_title('Figure 2-b  Satisfaction Levels in London')

# Adjust the display range to only show the distribution area of the points
xmin, ymin, xmax, ymax = gdf_points_new.total_bounds
axs[0].set_xlim([xmin, xmax])
axs[0].set_ylim([ymin, ymax])

xmin, ymin, xmax, ymax = gdf_quality.total_bounds
axs[1].set_xlim([xmin, xmax])
axs[1].set_ylim([ymin, ymax])

# Show the result
plt.show()
```

```{python}
# Filter out points that meet the conditions
red_points = gdf_points_new[(gdf_points_new['heat_level'] == 5) & (gdf_points_new['satisfaction_level'] == 1)]
red_points_all = gdf_points_new[(gdf_points_new['heat_level'] == 5)]
cyan_points = gdf_points_new[(gdf_points_new['heat_level'] == 1) & (gdf_points_new['satisfaction_level'] == 5)]
cyan_points_all = gdf_points_new[(gdf_points_new['satisfaction_level'] == 5)]

# Create the figure and subplots
fig, axs = plt.subplots(1, 2, figsize=(20, 10), sharex=True, sharey=True)

# Plot the boroughs as the base layer for both subplots
gdf_boroughs.plot(ax=axs[0], color='white', edgecolor='lightgrey')
gdf_boroughs.plot(ax=axs[1], color='white', edgecolor='lightgrey')

# Plot the red and cyan points on their respective subplots
red_points_all.plot(ax=axs[0], color='mistyrose', markersize=3)
red_points.plot(ax=axs[0], color='red', markersize=3)
cyan_points_all.plot(ax=axs[1], color='lightblue', markersize=3)
cyan_points.plot(ax=axs[1], color='dodgerblue', markersize=3)

# Set the titles for both subplots
axs[0].set_title('Figure 2-c  "Heat5" with highlighted "Satisfaction1"')
axs[1].set_title('Figure 2-d  "Satisfaction5" with highlighted "Heat1"')

# Determine the combined bounds of both datasets
xmin = min(red_points.total_bounds[0], cyan_points.total_bounds[0])
ymin = min(red_points.total_bounds[1], cyan_points.total_bounds[1])
xmax = max(red_points.total_bounds[2], cyan_points.total_bounds[2])
ymax = max(red_points.total_bounds[3], cyan_points.total_bounds[3])

# Adjust the display range to display only the distribution area of ​​the points
xmin, ymin, xmax, ymax = gdf_points_new.total_bounds
axs[0].set_xlim([xmin, xmax])
axs[0].set_ylim([ymin, ymax])

xmin, ymin, xmax, ymax = gdf_quality.total_bounds
axs[1].set_xlim([xmin, xmax])
axs[1].set_ylim([ymin, ymax])

# Ensure the aspect ratio is the same for both subplots
axs[0].set_aspect('equal', adjustable='box')
axs[1].set_aspect('equal', adjustable='box')

# Show the plots
plt.show()
```

```{python}
# red_points['price'] = red_points['price'].replace('[\$,]', '', regex=True).astype(float)
# print(red_points['price'].describe())

# red_points_all['price'] = red_points_all['price'].replace('[\$,]', '', regex=True).astype(float)
# print(red_points_all['price'].describe())

# cyan_points['price'] = cyan_points['price'].replace('[\$,]', '', regex=True).astype(float)
# print(cyan_points['price'].describe())

# cyan_points_all['price'] = cyan_points_all['price'].replace('[\$,]', '', regex=True).astype(float)
# print(cyan_points_all['price'].describe())
```

```{python}
# red_points = gdf_points_new[(gdf_points_new['heat_level'] == 5) & (gdf_points_new['satisfaction_level'] == 1)]
# cyan_points = gdf_points_new[(gdf_points_new['heat_level'] == 1) & (gdf_points_new['satisfaction_level'] == 5)]

# fig, ax = plt.subplots(figsize=(10, 10))

# gdf_boroughs.plot(ax=ax, color='white', edgecolor='lightgrey')
# red_points.plot(ax=ax, color='red', markersize=10, label='Heat: 5, Quality: 1')
# cyan_points.plot(ax=ax, color='cyan', markersize=10, label='Heat: 1, Quality: 5')

# ax.set_title('Special Points in London')

# xmin, ymin, xmax, ymax = gdf_points_new.total_bounds
# ax.set_xlim([xmin, xmax])
# ax.set_ylim([ymin, ymax])

# ax.legend()

# plt.show()
```

```{python}
# print(gdf_quality['satisfaction_level'].describe())
# print(gdf_points_new['heat_level'].describe())
```

```{python}
# Switch
```

## 7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how *could* this data set be used to inform the regulation of Short-Term Lets (STL) in London? 

Combining the previous classifications, we further analyze the listings to find factors related to hosts or listings that lead to inconsistencies or even reversals between popularity and satisfaction. We selected six items set by hosts to analyze the impact of host behavior. Figure 3 shows the relationship between the mean values of different items and different levels of listings. In addition to the mean values, the median or extremes also reflect similar trends. For clarity, only the mean values are plotted here.

```{python}
# data_2.info()
```

```{python}
# # ！！！！！！Since the running time of the subsequent code is too long, comment out the unnecessary parts here. If it needs to be run, it will take more than ten minutes.

# from sklearn.preprocessing import OneHotEncoder
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.feature_extraction.text import CountVectorizer
# from sklearn.decomposition import LatentDirichletAllocation
```

```{python}
# import nltk
# import spacy
# from nltk.corpus import wordnet as wn
# from nltk.stem.wordnet import WordNetLemmatizer
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize, sent_tokenize
# from nltk.tokenize.toktok import ToktokTokenizer
# from nltk.stem.porter import PorterStemmer
# from nltk.stem.snowball import SnowballStemmer
# from nltk import ngrams, FreqDist

# lemmatizer = WordNetLemmatizer()
# tokenizer = ToktokTokenizer()
```

```{python}
# # the method in the practical-07
# import urllib.request
# host  = 'https://orca.casa.ucl.ac.uk'
# turl  = f'{host}/~jreades/__textual__.py'
# tdirs = os.path.join('textual')
# tpath = os.path.join(tdirs,'__init__.py')

# if not os.path.exists(tpath):
#     os.makedirs(tdirs, exist_ok=True)
#     urllib.request.urlretrieve(turl, tpath)
```

```{python}
# from textual import *;
```

```{python}
# # select the houses which is 5 in heat_level and satisfaction_level
# heat5_house = data_2[data_2['heat_level'] == 5]
# satisfaction5_house = data_2[data_2['satisfaction_level'] == 5]
# analyzeheat5 = heat5_house

# # select the houses which is 4 in heat_level and satisfaction_level
# heat4_house = data_2[data_2['heat_level'] == 4]
# satisfaction4_house = data_2[data_2['satisfaction_level'] == 4]
# analyzeheat4 = heat4_house

# # select the houses which is 3 in heat_level and satisfaction_level
# heat3_house = data_2[data_2['heat_level'] == 3]
# satisfaction3_house = data_2[data_2['satisfaction_level'] == 3]
# analyzeheat3 = heat3_house

# # select the houses which is 2 in heat_level and satisfaction_level
# heat2_house = data_2[data_2['heat_level'] == 2]
# satisfaction2_house = data_2[data_2['satisfaction_level'] == 2]
# analyzeheat2 = heat2_house

# # select the houses which is 1 in heat_level and satisfaction_level
# heat1_house = data_2[data_2['heat_level'] == 1]
# satisfaction1_house = data_2[data_2['satisfaction_level'] == 1]
# analyzeheat1 = heat1_house

# highHeat_lowSat = data_2[(data_2['satisfaction_level']) == 1 & (data_2['heat_level'] == 5)]
# lowHeat_highSat = data_2[(data_2['satisfaction_level'])== 5 & (data_2['heat_level'] == 1)]
```

```{python}
# heatAll = [heat1_house,heat2_house,heat3_house,heat4_house,heat5_house]
# satisfactionAll = [satisfaction1_house,satisfaction2_house,satisfaction3_house,satisfaction4_house,satisfaction5_house]
# extreme = [highHeat_lowSat, lowHeat_highSat]
```

```{python}
# # It takes a long time from this code block to the last cell to appear.

# # Calculate the average of host_response_rate
# host_response_rate = []
# for i in heatAll:
#     j = i.dropna(subset=['host_response_rate'])
#     mean = j['host_response_rate'].str.rstrip('%').astype('float') / 100.0
#     host_response_rate.append(mean.mean())

# for i in satisfactionAll:
#     j = i.dropna(subset=['host_response_rate'])
#     mean = j['host_response_rate'].str.rstrip('%').astype('float') / 100.0
#     host_response_rate.append(mean.mean())
    
# for i in extreme:
#     j = i.dropna(subset=['host_response_rate'])
#     mean = j['host_response_rate'].str.rstrip('%').astype('float') / 100.0
#     host_response_rate.append(mean.mean())
```

```{python}
# # Calculate the average of host_acceptance_rate
# host_acceptance_rate = []
# for i in heatAll:
#     j = i.dropna(subset=['host_acceptance_rate'])
#     mean = j['host_acceptance_rate'].str.rstrip('%').astype('float') / 100.0
#     host_acceptance_rate.append(mean.mean())

# for i in satisfactionAll:
#     j = i.dropna(subset=['host_acceptance_rate'])
#     mean = j['host_acceptance_rate'].str.rstrip('%').astype('float') / 100.0
#     host_acceptance_rate.append(mean.mean())

# for i in extreme:
#     j = i.dropna(subset=['host_acceptance_rate'])
#     mean = j['host_acceptance_rate'].str.rstrip('%').astype('float') / 100.0
#     host_acceptance_rate.append(mean.mean())
```

```{python}
# # Calculate the average length of neighborhood_overview text
# neighborhood_overview = []
# for i in heatAll:
#     j = i.dropna(subset=['neighborhood_overview'])
#     j['neighborhood_overview'] = j.neighborhood_overview.apply(normalise_document, remove_digits=True)
#     textlist = j.neighborhood_overview.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     neighborhood_overview.append(averge)

# for i in satisfactionAll:
#     j = i.dropna(subset=['neighborhood_overview'])
#     j['neighborhood_overview'] = j.neighborhood_overview.apply(normalise_document, remove_digits=True)
#     textlist = j.neighborhood_overview.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     neighborhood_overview.append(averge)

# for i in extreme:
#     j = i.dropna(subset=['neighborhood_overview'])
#     j['neighborhood_overview'] = j.neighborhood_overview.apply(normalise_document, remove_digits=True)
#     textlist = j.neighborhood_overview.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     neighborhood_overview.append(averge)
```

```{python}
# # Calculate the average length of description text
# description = []
# for i in heatAll:
#     j = i.dropna(subset=['description'])
#     j['description'] = j.description.apply(normalise_document, remove_digits=True)
#     textlist = j.description.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     description.append(averge)

# for i in satisfactionAll:
#     j = i.dropna(subset=['description'])
#     j['description'] = j.description.apply(normalise_document, remove_digits=True)
#     textlist = j.description.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     description.append(averge)

# for i in extreme:
#     j = i.dropna(subset=['description'])
#     j['description'] = j.description.apply(normalise_document, remove_digits=True)
#     textlist = j.description.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     description.append(averge)
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
# # Calculate the average length of host_about text
# host_about = []
# for i in heatAll:
#     j = i.dropna(subset=['host_about'])
#     j['host_about'] = j.host_about.apply(normalise_document, remove_digits=True)
#     textlist = j.host_about.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     host_about.append(averge)

# for i in satisfactionAll:
#     j = i.dropna(subset=['host_about'])
#     j['host_about'] = j.host_about.apply(normalise_document, remove_digits=True)
#     textlist = j.host_about.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     host_about.append(averge)

# for i in extreme:
#     j = i.dropna(subset=['host_about'])
#     j['host_about'] = j.host_about.apply(normalise_document, remove_digits=True)
#     textlist = j.host_about.tolist()
#     total = 0
#     for i in textlist:
#         total = total + len(i)
#     averge = total/len(textlist)
#     host_about.append(averge)
```

```{python}
# # Calculate the average of price
# price= []
# for i in heatAll:
#     i['price'] = i['price'].str.replace('$','', regex=False).str.replace(',','', regex=False).astype('float')
#     mean =  i.price.mean()
#     price.append(mean)

# for i in satisfactionAll:
#     i['price'] = i['price'].str.replace('$','', regex=False).str.replace(',','', regex=False).astype('float')
#     mean =  i.price.mean()
#     price.append(mean)

# for i in extreme:
#     i['price'] = i['price'].str.replace('$','', regex=False).str.replace(',','', regex=False).astype('float')
#     mean =  i.price.mean()
#     price.append(mean)
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
# # Creating df table
# row_list = ['heat1','heat2','heat3','heat4','heat5',
#             'sat1','sat2','sat3','sat4','sat5',
#             'heat5_sat1', 'heat1_sat5']

# dir1 = { 'Level': row_list,
#         'price(£)': price,
#         'host_response_rate(%)': host_response_rate,
#         'host_acceptance_rate(%)': host_acceptance_rate
#       }

# dir2 = { 'Level': row_list,
#         'neighborhood_overview': neighborhood_overview,
#         'description': description,
#         'host_about': host_about
#       }
```

```{python}
# # format DataFrame1
# formatted_df1 = pd.DataFrame(dir1)

# # Format columns
# formatted_df1['price(£)'] = formatted_df1['price(£)'].round(2)

# # Convert decimal to percent with two decimal places
# formatted_df1['host_response_rate(%)'] = (formatted_df1['host_response_rate(%)'] * 100).round(2)
# formatted_df1['host_acceptance_rate(%)'] = (formatted_df1['host_acceptance_rate(%)'] * 100).round(2)

# # format DataFrame2
# formatted_df2 = pd.DataFrame(dir2)

# # Convert column to integer
# formatted_df2['neighborhood_overview'] = formatted_df2['neighborhood_overview'].astype(int)
# formatted_df2['description'] = formatted_df2['description'].astype(int)
# formatted_df2['host_about'] = formatted_df2['host_about'].astype(int)
```

```{python}
# formatted_df1.to_csv('formatted_df1.csv', index=False)
# formatted_df2.to_csv('formatted_df2.csv', index=False)
```

```{python}
df_savetime1 = pd.read_csv('formatted_df1.csv')
df_savetime2 = pd.read_csv('formatted_df2.csv')
```

```{python}
# # Set up Styler to format tables
# df_styled1 = df_savetime1.style.format({
#     'price(£)': "{:.2f}",
#     'host_response_rate(%)': "{:.2f}",
#     'host_acceptance_rate(%)': "{:.2f}",
# }).set_properties(**{'text-align': 'center'})

# df_styled1.hide(axis='index')
```

```{python}
# # Set up Styler to format tables
# df_styled2 = df_savetime2.style.format({
#     'neighborhood_overview': "{:.0f}",
#     'description': "{:.0f}",
#     'host_about': "{:.0f}"
# }).set_properties(**{'text-align': 'center'})

# df_styled2.hide(axis='index')
```

```{python}
df1 = df_savetime1.copy()
df1.set_index('Level', inplace=True)

# Create a bar chart
ax = df1[['price(£)']].plot(kind='bar', figsize=(8, 5), color=['#a8dadc'])

# Set title and labels
ax.set_title('Figure 3-a  Price', fontsize=16)
ax.set_xlabel('Level', fontsize=12)
ax.set_ylabel('Count', fontsize=12)

# Set the x-axis labels to be tilted at 45 degrees
plt.xticks(rotation=45)

# Set the range of the y-axis
ax.set_ylim(0, 400)

# Display the legend
ax.legend(title='Legend')

# Show the plot
plt.tight_layout()
plt.show()
```

```{python}
df2 = df_savetime1.copy()
df2.set_index('Level', inplace=True)

# Create a bar chart
ax = df2[['host_response_rate(%)', 'host_acceptance_rate(%)']].plot(kind='bar', figsize=(8, 5), color=['#0077b6', '#90e0ef'])

# Set title and labels
ax.set_title('Figure 3-b  Response rate and acceptance rate', fontsize=16)
ax.set_xlabel('Level', fontsize=12)
ax.set_ylabel('Count', fontsize=12)

# Set the x-axis labels to be tilted at 45 degrees
plt.xticks(rotation=45)

# Set the range of the y-axis
ax.set_ylim(70, 120)

# Display the legend
ax.legend(title='Legend')

# Show the plot
plt.tight_layout()
plt.show()
```

```{python}
df3 = df_savetime2.copy()
df3.set_index('Level', inplace=True)

# Create a bar chart
ax = df3.plot(kind='bar', figsize=(8, 5), color=['#023e8a', '#00b4d8', '#ade8f4'])

# Set title and labels
ax.set_title('Figure 3-c  The length of three types of descriptive text', fontsize=16)
ax.set_xlabel('Level', fontsize=12)
ax.set_ylabel('Count', fontsize=12)

# Set the x-axis labels to be tilted at 45 degrees
plt.xticks(rotation=45)

# Set the range of the y-axis
ax.set_ylim(0, 750)

# Display the legend
ax.legend(title='Legend')

# Show the plot
plt.tight_layout()
plt.show()
```

- Figure 3-a:
The average price of Heat1_sat5 listings is the highest, indicating that price is a significant factor hindering people from placing orders. The average price of high-popularity and low-satisfaction listings is the lowest, indicating that many people are attracted by low prices, even if they are dissatisfied.

- Figure 3-b:
The mean reply rates for different levels are all above 90%, with little difference. The relationship between satisfaction and reply rate is not obvious. But the lower the acceptance_rate, the higher the satisfaction. This indicates that on the Airbnb platform, guests and hosts are mutually selective, and “picky” hosts often have better listings.

- Figure 3-c:
The analysis here focuses on the lengths of three descriptive texts. The description of the listing is the longest, then the surroundings, and lastly, the host's self-introduction. There is not a significant change in these lengths between different groups, but interestingly, the higher the popularity, the shorter the self-introduction. May this mean that long with shorter self-introductions are less popular? Conclusions are difficult to draw based solely on length.

Focus on analyzing two extreme groups. In Heat5_sat1, both the description and self-introduction lengths are significantly smaller than Heat1_sat5, indicating that hosts with more descriptions are more likely to achieve high satisfaction. However, the description of the surroundings is the opposite; perhaps the high popularity location deserves more description. In fact, drawing effective conclusions through text length analysis is difficult. And using high-frequency keywords does not lead to effective conclusions as well. For example, the top three keywords are "room," "space," and "London." Due to technological limitations, NLP analysis will be considered for future work.

### Conclusion

Among the factors affecting the "mismatch of popularity and satisfaction," we found that host behavior has a considerable impact. There may be significant differences between "professional hosts" and "hosts who meet the original intention of Airbnb" in terms of relevant abilities and awareness.

By identifying such mismatches, the Airbnb platform can optimize its supervision mechanism.

- Penalties can be imposed on hosts who rely on unreal descriptions for marketing.

- Further optimize the way listings are created, providing more multiple-choice questions rather than subjective questions.

- In addition to spreading legal knowledge, host strategy tutorials or photo-taking advice can also be provided.

The previous analysis also shows that hotels with low scores and low prices are highly popular. It is necessary to admit that each price has its corresponding quality, but low scores mean that the quality has fallen below the customer's expectations. 

This may require the intervention of government regulations. For example, certain defects must be clearly described, and the description of certain items must correspond to facility standards. However, it should also be noted that Airbnb is more like a second-hand market compared to the B2C in the hotel industry. Their individuality and freedom must be protected. The regulations should serve as a safety-net mechanism. This ensures that the quality of each listing is predictable and provides guests with a personalized experience.

## References
